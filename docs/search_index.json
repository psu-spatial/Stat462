[
["index.html", "STAT-462 Lab book Chapter 1 Tutorials 1.1 Tutorial 1A: Installing R and R-studio 1.2 Tutorial 1B: Creating and loading R projects 1.3 Tutorial 1C: R coding basics 1.4 Tutorial 1D: Packages 1.5 Tutorial 1E: R Markdown basics 1.6 Tutorial 2A: Getting help in R 1.7 Tutorial 2B: Markdown formats 1.8 Tutorial 2C: Selecting &amp; summarising data 1.9 Tutorial 2D: Distributions and tests 1.10 Tutorial 3A - Editing code chunks 1.11 Tutorial 3B - Regression Models 1.12 Tutorial 4A: Plotting in ggplot2 1.13 Tutorial 4B: Identifying/removing outliers 1.14 Tutorial 4C Writing equations in R-Studio 1.15 Tutorial 5A ANOVA 1.16 Tutorial 5B Diagnostics part A 1.17 Tutorial 5C Confidence and Prediction Intervals 1.18 Tutorial 6A Diagnostics part B: Outliers 1.19 Tutorial 6B Transformations 1.20 Tutorial 6C Basic model comparisons 1.21 Tutorial 7A Covariance matrices 1.22 Tutorial 7B Mulitple Regression Models 1.23 Tutorial 7C Residuals in MLR 1.24 Tutorial 7D Prediction and Confidence Intervals in MLR", " STAT-462 Lab book Helen Greatrex 2021-04-01 Chapter 1 Tutorials 1.1 Tutorial 1A: Installing R and R-studio In this class, we would like you to download and install BOTH R and R-Studio onto your own computers. You should have already completed this step for Homework 1. If you have not, see instructions here: https://psu.instructure.com/courses/2115020/assignments/12682456 FOR WINDOWS USERS. Sometimes R asks you to download something called RTools. You can do so here: https://cran.r-project.org/bin/windows/Rtools/ Follow the instructions closely and ask if you need support. 1.1.1 Open R-studio Figure 1.1: Icons of the R program and R-studio Open R-studio (NOT R!). You should be greeted by three panels: The interactive R console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help/Viewer (tabbed in lower right) If you click on the View/Panes/Pane Layout menu item, you can move these quadrants around. I tend to like the console to be top left and scripts to be top right, with the plots and environment on the bottom - but this is totally personal choice. Figure 1.2: The basic R-studio screen when you log on If you wish to learn more about what these windows do, have a look at this resource, from the Pirates Guide to R: https://bookdown.org/ndphillips/YaRrr/the-four-rstudio-windows.html If you have used R before, you might see that there are variables and plots etc already loaded. It is always good to clear these before you start a new analysis. To do this, click the little broom symbol in your environment tab. 1.1.2 Change a few settings R-studio wants to be helpful and will try to re-load exactly where you were in a project when you log back in. This can get really confusing, so we are going to turn this off. ON A MAC: Click on the R-studio menu button on the top left of the screen, then click Preferences. ON A PC: Click on Tools-&gt; Global Options -&gt; Preferences Now: UNCLICK “Restore most recently opened project at startup” Set “Save workspace to .RData on” exit to Never 1.2 Tutorial 1B: Creating and loading R projects 1.2.1 What are projects? One of the most useful new features in RStudio are R-Projects. We are going to use R-projects to store our lab data in. An R project is a place to store all your commands, data and output in the same place. Before R projects, we had to worry about where on your computer your data was saved in. Now, we can force R-Studio to immediately look inside your lab folder - making life much easier. Equally, projects allow you to have multiple versions of R-studio open. This is useful because say you have Lab-2 open, but want to re-open Lab-1 to check some code, clicking the “Lab 1 project file” will load a whole new version of R with Lab 1 ready to edit. It is really important that you stay organized in this course, for example making sure you know where you put your data and your labs. To encourage this, choose a place that makes sense on your computer, create a folder called STAT-462. can either do this outside R studio in your file explorer, or inside the “Files tab” in R studio. You should store all your 462 labs inside this folder Now we will create our first project (also see the pic): open R-Studio and click New Project, then click “new directory” then “new project”. Name your project Lab 1, then browse to your newly created STAT-462 folder and press open. Press OK and your project is created. You will also see that a new sub-folder has been created in your STAT-462 folder called Lab 1 Inside that is your Project file. (with a .Rproj extension) Figure 1.3: Instructions to create an R project Your output should look something like this: Figure 1.4: What you should see Equally, R should now be “looking” inside your Lab 1 folder, making it easier to find your data and output your results. Try typing this into the console (INCLUDING THE EMPTY PARANTHESES/BRACKETS) and see if it prints out the location of Lab 1 on your computer. If not, talk to an instructor. getwd() In the future, every time you want to work on Lab 1, rather than open R-studio directly, double click the R project file inside Lab 1 and you will get back to your work. If you’re having issues at this point or haven’t managed to get to this step, STOP! Ask an instructor for help. Now you have opened your project, take a screenshot of your R-studio page. It should look like Figure 1.4, e.g. with at least R version 4.0.3, with the Lab 1 project and the project stored in your STAT-462 folder. To take a screenshot on a mac, press Command-3. The screenshot will appear on your desktop To take a screenshot on a PC, press Alt + PrtScn Rename the screenshot to your “username_Lab1_Fig1”(for example for me it will be hlg5155_Lab1_Fig1), then place it in your Lab 1 sub-folder inside STAT-462. This folder was created when you made the project. You will need this later, so don’t skip this step. 1.3 Tutorial 1C: R coding basics So now we FINALLY get to do some R-coding. First things first, first watch the 5 min video above for some pointers. Will will also go through this below: 1.3.1 Using R as a calculator Remember that the aim of R is to type in commands to get your computer to analyse data. The console (see Figure 1.2) is a space where you can type in those commands and it will directly print out the answer. You’re essentially talking to the computer. The little “&gt;” symbol in the console means that the computer is waiting for your command. Let’s start by the simplest command possible. Try typing each of the following commands into your R console and pressing Enter as you work through this. 1+1 ## [1] 2 Note that spacing does not matter: 1+1 will generate the same answer as 1 + 1. When using R as a calculator, the order of operations is the same as you would have learned back in school, so use brackets to force a different order. For example, 3 + 5 * 2 ## [1] 13 will give a different result to (3 + 5) * 2 ## [1] 16 1.3.2 Adding text Now for text. Can you say hello world? Figure 1.5: Your screen after running the project Nope, there is an error! To make R understand text, it is important to use quote marks. print(&quot;Hello World&quot;) ## [1] &quot;Hello World&quot; 1.3.3 Comparisons We can also do comparisons in R - using the special symbols TRUE or FALSE (no quote marks, they are special). Here we are asking R whether 1 is equal to 1. # note two equals signs is read as &quot;is equal to&quot; 1 == 1 ## [1] TRUE We could also have used != “Not equal to” &lt; “Less than” &lt;= \"Less than or equal to` &gt; “Greater than” &gt;= “Greater than or equal to” 1.3.4 What if I press Enter too soon? If you type in an incomplete command, R will wait for you to complete it. For example, if you type 1 + and press enter, R will know that you need to complete the command So it will move onto the next line but the &gt; will have changed into a +, which means its waiting for you to complete the command. If you want to cancel a command you can simply hit the “Esc” key or press the little stop symbol and R studio will reset. Pressing escape isn’t only useful for killing incomplete commands: you can also use it to tell R to stop running code (for example if it’s taking much longer than you expect), or to get rid of the code you’re currently writing. 1.3.5 Variables and assignment It’s great to be able to do maths easily on the screen, but really we want to be able to save our results, or load in data so we can run more complex commands. In R, we can give our data a name e.g. we save our data as a variable. So then, instead of typing the whole command, we can simply type the variable’s name and R will recall the answer. The symbol to store data into a variable is using the assignment arrow &lt;-, which is made up of the left arrow and a dash. You can also use the equals sign, but it can cause complications later on. Try typing this command into the console: x &lt;- 1/50 Notice that pressing enter did not print a value onto your screen as it did earlier. Instead, we stored it for later in something called a variable, with the name ‘x’. So our variable x is now associated with the value 0.02, or 1/50. You can print a variable on screen by typing its name, no quotes, or by using the print command. Try printing out your variable. x ## [1] 0.02 # or print(x) ## [1] 0.02 Look for the Environment tab in one of the panes of RStudio, and you will see that ‘x’ and its value have appeared. This ‘x’ variable can be used in place of a number in any calculation that expects a number. Try typing log(x) ## [1] -3.912023 Notice also that variables can be reassigned: x &lt;- 100 print(x) ## [1] 100 x used to contain the value 0.025 and and now it has the value 100. Note, the letter x isn’t special in any way, it’s just a variable name. You can replace it with any word you like as long as it contains no spaces and doesn’t begin with a number. Different people use different conventions for long variable names, these include periods.between.words.1 underscores_between_words camelCaseToSeparateWords What you use is up to you, but be consistent. Finally, R IS CASE SENSITIVE. X and x are different variables! h &lt;- 1 H &lt;- 2 ans &lt;- h+H print(ans) ## [1] 3 1.3.5.1 Combining variables As I showed above, you can now use multiple variables together in more complex commands. For example, try these commands: #Take the variable x, add 1 then save it to a new variable called y y &lt;- x + 1 # print the multiple of 2yx onto the screen y ## [1] 101 Now you can see that there are two variables in your environment tab, x and y. Where y is the sum of the contents of x plus 1. The way R works is that first it looks for the commands on the right of the arrow. It runs all of them, calculates the result, then saves that result with the name on the left of the arrow. It does not save the command itself, just the answer. For example, in this case, R has no idea that y was created using a sum, it just knows that it is equal to the number 3. You can even use this to change your original variable . Try typing the code below in a few times into the console and see what happens. A short cut to do this is to type the commands the first time, then use the up-arrow on your keyboard to cycle back through previous commands you have typed x &lt;- x + 1 # notice how RStudio updates its description of x in the environment tab x # print the contents of &quot;x&quot; onto the screen ## [1] 101 Our variables don’t have to be numbers. They could refer to tables of data, or a spatial map, or any other complex thing. We will cover this more in future labs. 1.3.6 Functions A command is simply an action you can take - like pressing the square root button on a calculator, followed by the number you wish to take the square root of. A command is always followed by parentheses ( ), inside which you put your arguments. The power of R lies in its many thousands of these built in commands, or functions. In fact, we have already come across one - the print command. Some more examples include: plot(x=1:10,y=1:10) This will plot the numbers 1 to 10 against 1 to 10 x &lt;- nchar(\"hello\") This will count the number of letters in the word “hello” (e.g. 5), then save it as a variable called x Watch this short video to learn three important facts about functions: One example of a function is file.choose() (not how I put the parentheses in this lab book so you can see it is a command). This command will let you interactively select a file and print the address out onto the screen. Try each of these out in your console for the file.choose() command, leaving the parentheses blank. # Typing this into the console will print out the underlying code file.choose # Typing it WITH parentheses will run the command. file.choose() # Typing a ? in front will open the help file for that command ?file.choose Sometimes we need to give the command some additional information. Anything we wish to tell the command should be included inside the inside the parentheses (separated by commas). The command will literally only know about the stuff inside the parentheses. sin(1) # trigonometry functions. Apply the sine function to the number 1. ## [1] 0.841471 log(10) # natural logarithm. Take the natural logarithm of the number 10. ## [1] 2.302585 This following command will plot the number 1 to 10 against the numbers 12 to 20, along with some axis labels. When you run this, the plot will show up in the plots tab. # plot the numbers 1 to 10 against the numbers 11 to 20 plot(1:10,11:20,col=&quot;dark blue&quot;, xlab=&quot;x values&quot;,ylab=&quot;STAT-462 is the best&quot;) If you are feeling lost, https://swcarpentry.github.io/r-novice-gapminder/01-rstudio-intro/ is a really good website which goes over a lot of this in more detail. A lot of this is based on their work. 1.4 Tutorial 1D: Packages There are now several million commands/functions available for you to use in R. To make sure your computer doesn’t get overwhelmed, it doesn’t load all of these at once. In fact many need to be downloaded from the internet. So we have R: The programming language itself Functions: Specific commands or actions written in the R language Packages: Commands are grouped into bundles/apps called packages, which we download from the internet and load every time we need them. A close analogy is your phone. There are millions of apps available from banking, to social media to camera filters. You don’t have every app in the world installed on your phone - and you don’t have every app you do download running at the same time. This is the same for R: You download and install packages from the internet that you might need. This can be done by clicking the install button in the Packages tab. Or you can use the install.packages() command. You only ever need to do this once. To actually run the commands in the package you need to load/run them - just in the way you tap an app to start it. This can be done using the library() command. Now we are going to download some packages from the internet and install them. You must be connected to the internet to make this happen! In the console, type the following commands, or click the “Install” button in the packages tab (next to plots) and find the package name. If it asks if you want to install dependencies, say yes. If R gives you warnings about rtools, ignore them or follow the instructions at the top to install R-Tools. # COPY/PASTE THESE INTO YOUR CONSOLE AND LET THEM RUN. #(These are hopefully all the packages for the course, so it&#39;s a one off download) install.packages(&quot;tidyverse&quot;) # Lots of data processing commands install.packages(&quot;knitr&quot;) # Helps make good output files install.packages(&quot;rmarkdown&quot;) # Helps make good output files install.packages(&quot;lattice&quot;) # Makes nice plots install.packages(&quot;RColorBrewer&quot;) # Makes nice color-scales install.packages(&quot;ISLR&quot;) # contains a credit dataset install.packages(&quot;yarrr&quot;) # contains a toy dataset about pirates install.packages(&quot;skimr&quot;) # Summary statistics install.packages(&quot;Stat2Data&quot;) # Regression specific commands install.packages(&quot;olsrr&quot;) # Regression specific commands install.packages(&quot;nortest&quot;) # Regression specific commands install.packages(&quot;lmtest&quot;) # Regression specific commands install.packages(&quot;IMTest&quot;) # Regression specific commands install.packages(&quot;MASS&quot;) # Regression specific commands install.packages(&quot;moderndive&quot;)# Regression specific commands install.packages(&quot;corrplot&quot;) # correlation plots install.packages(&quot;ggpubr&quot;) # Nice regression plots install.packages(&quot;car&quot;) # this one sometimes has problems, don&#39;t panic if you get errors You will see a load of red text appear in the console as it tells you its very detailed status of how it’s downloading and installing. Don’t panic! It might also take several minutes to do this, longer on a bad internet connection. We are doing this as a one off at the start of the course. When you have run all the commands and waited until they have finished running (remember, when it is done, you should see the little “&gt;” symbol in the console waiting for your next command), we want to check if they have installed successfully onto your computer. To do this we are going to load them using the library command: library(&quot;tidyverse&quot;) # Lots of data processing commands library(&quot;knitr&quot;) # Helps make good output files library(&quot;rmarkdown&quot;) # Helps make good output files library(&quot;lattice&quot;) # Makes nice plots library(&quot;RColorBrewer&quot;) # Makes nice color-scales library(&quot;ISLR&quot;) # contains a credit dataset library(&quot;yarrr&quot;) # contains a toy dataset about pirates library(&quot;skimr&quot;) # Summary statistics library(&quot;Stat2Data&quot;) # Regression specific commands library(&quot;olsrr&quot;) # Regression specific commands library(&quot;nortest&quot;) # Regression specific commands library(&quot;lmtest&quot;) # Regression specific commands library(&quot;IMTest&quot;) # Regression specific commands library(&quot;MASS&quot;) # Regression specific commands library(&quot;moderndive&quot;)# Regression specific commands library(&quot;corrplot&quot;) # correlation plots library(&quot;ggpubr&quot;) # Nice regression plots library(&quot;car&quot;) # this one sometimes has problems, don&#39;t panic if you get errors If you have managed to install them successfully, often nothing happens - this is great! It means it loaded the package without errors. Sometimes, it will tell you friendly messages. For example, this is what shows up when you install the tidyverse package. It is telling you the sub-packages that it downloaded and also that some commands, like filter - now have a different meaning. E.g. originally the filter command did one thing, but now the tidyverse package has made filter do something else. Figure 1.6: Tidyverse install messages To find out if what you are seeing is a friendly message or an error, run the command again. If you run it a second time and there is no error then nothing should happen. ! IMPORTANT If you see some red errors here after multiple attempts running the commands, we will have to fix what your computer is doing together. If you see errors, then take a screenshot of the full page and talk to a TA or Dr Greatrex, or post on the Lab 1 discussion. Note - you don’t need to submit anything for lab challenge 2 - it’s here to make the rest of the semester run smoothly. Your only action is to reach out if there are errors. 1.5 Tutorial 1E: R Markdown basics 1.5.1 What is R-markdown? So far, we’ve been typing commands into the console, but these will all be lost once you close R. Equally, it’s hard to remember or reproduce the analysis you have done . So we will now move onto writing code commands that you can save and submit. There are several types of document that you can create and save in R-Studio. A basic script (the filetype is .r). This is simply just a blank document where you can save code commands. When you “run” the commands in the script, R simply copy/pastes the commands over to the console. An R-Notebook or R-Markdown document (the filetype is .Rmd). These are documents you can use to write a report with normal text/pictures, but and also include both your R code and output. You can turn these into reports, websites, blogs, presentations or applications. For example these instructions are created using a markdown document. In this course we are going to focus on the R-Markdown format and you are going to submit your labs as html files. 1.5.2 Creating a markdown document Time to make your own. Go to the File menu on the top left, then click New File - R-Markdown. It will ask you to name and save your file. Call it STAT-462 Lab 1. Figure 1.7: You should see TWO new files appear in your lab 1 folder A file should appear on your screen - your first markdown script. Essentially, we have some space for text, some space for code, and a space at the top of the file where we can add information about themes/styles etc. Each file contains some friendly text to explain what is going on, which I have annotated here. Figure 1.8: You should see TWO new files appear in your lab 1 folder Code chunks are the grey areas and are essentially “mini consoles”. If you click on the little right triangle arrow at the top-right of the code chunk, you can run the plot command, and a plot will appear, running the code. Note, it no longer runs in the console. You can still copy things into the console, by clicking on the line you want to run and pressing Ctrl-Enter / command-Enter. Let’s try this. On line 11, delete plot(cars) and type 1+1. Now press the green arrow and the answer should appear directly under your code chunk. Now click at the end of the script, around line 20 and press the green insert button (top right in the script menu icons). Insert a new R code chunk. Inside, type 1+2 and run it. So you can see that you can have text/pictures/videos etc, and code chunks interspersed throughout it. Now press the “knit” button (right next to the save button). It will first ask you to save. Save it as STAT462_Lab1_PSU.ID e.g. STAT462_Lab1_hlg5155. Then, this should make a new file pop-up, a html pretty version of your code and output. If you go to your lab 1 folder, you will see that this has appeared next to your .Rmd file. 1.5.3 “Friendly text” Much of what you see on your screen when you open a notebook document is simply a friendly introduction to RStudio. So your notebook file is essentially a Word document with the ability to add in “mini R-consoles”, AKA your code chunks. Imagine every time you opened a new word document, it included some “friendly text” (“hi, this is a word document, this is how you do bold text, this is how you save”). This is great the first time you ever use Word, but kind of a pain after that. RStudio actually does this. Every time you open a notebook file, you actually do get this friendly text, explaining how to use a notebook file. Read it, make sure you understand what it is telling you, then delete all of it. So delete from line 6 to the end. The stuff left over is your YAML code which tells R how to create the final html file. DON’T TOUCH THAT. Figure 1.9: You should see TWO new files appear in your lab 1 folder 1.6 Tutorial 2A: Getting help in R There are going to be many occasions when either you are not sure what to do in R or something is not working. Here are some of the places I get help: 1.6.1 Help on a specific command You can type ?function_name into the console where [name] is the CASE SENSITIVE name of the command. For example, you can find help on the boxplot command by typing ?boxplot or help(boxplot). The top part of each help file will show you a list of the parameters that you can use to modify the command. For example in the boxplot help file you can see that the col parameter can be used to change the boxplot colour. The middle part of the help file contains details about the command for example the background or further reading The last part of the help file contains worked examples that you can literally copy/paste into the console too see how they work. If you’re sure you typed the name correct but it can’t find the help-file, then you probably first need to load the package containing the command. For example ?ggqqplot will not work before you use library(\"ggpubr\") to load the ggpubr package. 1.6.2 Finding a command name Let’s say that you want to run a Wilks Shapiro test but you cannot remember what the command is called. In the console try typing ??\"search_text\" or help.search(\"search_text\") e.g. try ??\"shapiro\" or help.search(\"shapiro\"). Note, there are now quote marks around the text you are searching for. To seek help on special operators, use quotes or back-ticks e.g. ?\"&lt;-\". help.search(\"shapiro\") will search through every R help file on your computer that includes the word “shapiro”. You can see that there are a three commands I can choose from (I might have more than you here). For each command you can see the package containing the command, followed by :: then the command name. Clicking on it brings up the help file. Remember to actually use any command in R, you first need to load the relevant package. For example, if I wanted to use the nortest version of sf.test, then I would first need to include the command library(nortest) in order for it to be available. 1.6.3 Asking the internet: Figure 1.10: This may be a cartoon but it’s genuinely how I debug my code! If your code isn’t working. Google is a great tool and you have full permission to search google to debug your code - I guarantee someone will have seen your error code before. I often literally copy/paste an error code into google and see what comes up. Or search google for things like “how to add an image R markdown”. One good place to search is Stack Overflow. To go directly to R-related topics, visit http://stackoverflow.com/questions/tagged/r. Here’s how to ask a good question: https://stackoverflow.com/help/how-to-ask When asking for help, typing the command sessionInfo() into the console will print out the version of R and the packages you have loaded. This can be useful for debugging any issues. You also always have lab sessions to ask questions or the discussion boards at any time. 1.6.4 General help &amp; cheatsheets Within R, try running help.start() in the console (remember to include the empty parentheses or R won’t realise it’s a command). This brings up all the manuals R has available on your computer. An introduction to R is particularly useful. Click on it and have a look around. There are also some really good cheat-sheets out there: MARKDOWN cheat-sheet: go to the help menu at the very top of your screen. Click Markdown Quick Reference. Loads more cheatsheets: https://rstudio.com/resources/cheatsheets/ The R graph gallery https://www.r-graph-gallery.com/ contains code for many beautiful plots that you can make. Googling the name of a package often leads to a website with nice tutorials. For example, I found this for the paletteer package https://emilhvitfeldt.github.io/paletteer/. A nice markdown summary - https://ourcodingclub.github.io/tutorials/rmarkdown/ with answers to things like preview vs knit. 1.7 Tutorial 2B: Markdown formats The aim of this tutorial is to make your markdown reports more professional and easy to read. In future weeks we will continue to expand this work so that by the end of the course, you can make highly professional interactive reports. For example, click here for all the types of document you could easily turn your report into. 1.7.1 Editing YAML code This is the code at the top of your file (Figure 1.9). It controls how your final output looks and which type of output it becomes. When you create a markdown file, your code will look like one of these two options (one created using notebook and one markdown - either are fine) Figure 1.11: One created using a notebook and one using a markdown No matter what you have, it is useful to change the YAML code so that it looks like this, but with your name &amp; Lab 2. Note, if you copy/paste this and it doesn’t work, sometimes the quote marks copy weirdly from the internet - try deleting and retyping the quotes. --- title: &quot;STAT-462: Lab 2&quot; author: &quot;Helen Greatrex&quot; date: &quot;`r Sys.Date()`&quot; output: html_document: toc: true toc_float: yes number_sections: yes theme: lumen df_print: paged html_notebook: toc: true toc_float: yes number_sections: yes theme: lumen --- Now, save your file, or click “preview” at the top of the script. See if it works. There is a reasonable chance this won’t work first time around, as editing the YAML code can be a pain. It is very case and space sensitive. For example, the spaces at the start of some lines are important and are created using the TAB KEY, not the space bar. There is one TAB key before html_notebook (which is now on a new line). There are two TAB KEYS before toc, toc_float, number_sections and theme. Don’t continue until you can make and view your html or nb.html file. If it doesn’t work, ask for help before moving on The elements we just added are: The title The author Today’s date A floating table of contents and numbered sections (this won’t appear until you start typing section headings) The document is now in the lumen theme. You can choose other themes for markdown documents here. https://www.datadreaming.org/post/r-markdown-theme-gallery/ There are many more details about different options you can add here: https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf If you want to get really fancy, there is an interesting package to help you design YAML code here: https://education.rstudio.com/blog/2019/10/tools-for-teaching-yaml-with-ymlthis/ 1.7.2 Text formats R-Markdown uses text formatting a bit like LateX rather than word where you format the text as you write. First off - there are many cheat-sheets out there, but I particularly like this one https://www.markdownguide.org/basic-syntax/. One important way to make your R Markdown documents look more professional is to add in headings, sub-headings and text formats. Today we will look at headings and bold/italic text. Figure 1.12: A typical Rmd file (right) and its output (left) - note you need to click on the contents bar to expand it First, go to the help menu at the very top of your screen (might be hidden) and click Markdown quick reference. This will bring up a cheat-sheet. Figure 1.13: The top menu might be hidden Paragraphs and white space - READ THIS: R is very sensitive to blank white lines. Put them everywhere. Put a blank line between paragraphs, before/after headings, before lists, before/after code chunks…. If your formatting isn’t working well, chances are there isn’t a blank line before or after it. Headings: Inside a code chunk, the # symbol allows you to write mini comments that help explain the command e.g. # calculate 1+1 and 2+2 1+1 ## [1] 2 2+2 ## [1] 4 Outside a code chunk, the # symbol makes a heading, AS LONG AS YOU HAVE A WHITE LINE ABOVE AND BELOW. Have a look at Figure 1.13 for an example. Including headings this way allows automatic section numbering and will allow the table of contents to automatically be created. In the script itself the headings won’t show up - you will only see them when you press knit. Bold and italic: Putting stars or _ around your text will make it bold or italic (or both). Again have a look at Figure 1.13 and the cheat-sheet. Interestingly (and annoyingly), there is no simple way to underline text. In the script itself the text formatting won’t show up, you will only see them when you press knit. 1.8 Tutorial 2C: Selecting &amp; summarising data 1.8.1 Data.frame introduction Most of the data we will look at is in “data.frame” format. This is a table, just like an excel spreadsheet, with one row for each observation and one column for each variable. Each column has a column name. Next week we will work on loading our own data into R. This week we will focus on in-built R datasets. Let’s choose one now. I’m going to work with the pirates dataset from the yarrr package. We can choose the data here. library(yarrr) library(tidyverse) ?pirates dataset &lt;- pirates To have a look at the data there are many options. You can either: click on its name in the environment tab Type its name into the console or into a code chunk (e.g. for our table, type dataset into the console or a code chunk) Run the command View(variable_name) (View is a command from the tidyverse package) Run the command print(variable_name) Run the command head(variable_name) to see the first 6 lines or so (good for quick checks) For example # Note, there are more columns to the right, use the arrow to see head(dataset) ## id sex age height weight headband college tattoos tchests parrots ## 1 1 male 28 173.11 70.5 yes JSSFP 9 0 0 ## 2 2 male 31 209.25 105.6 yes JSSFP 9 11 0 ## 3 3 male 26 169.95 77.1 yes CCCC 10 10 1 ## 4 4 female 31 144.29 58.5 no JSSFP 2 0 2 ## 5 5 female 41 157.85 58.4 yes JSSFP 9 6 4 ## 6 6 male 26 190.20 85.4 yes CCCC 7 19 0 ## favorite.pirate sword.type eyepatch sword.time beard.length ## 1 Jack Sparrow cutlass 1 0.58 16 ## 2 Jack Sparrow cutlass 0 1.11 21 ## 3 Jack Sparrow cutlass 1 1.44 19 ## 4 Jack Sparrow scimitar 1 36.11 2 ## 5 Hook cutlass 1 0.11 0 ## 6 Jack Sparrow cutlass 1 0.59 17 ## fav.pixar grogg ## 1 Monsters, Inc. 11 ## 2 WALL-E 9 ## 3 Inside Out 7 ## 4 Inside Out 9 ## 5 Inside Out 14 ## 6 Monsters University 7 To see what the column names are, you can use the names(dataset) command names(dataset) ## [1] &quot;id&quot; &quot;sex&quot; &quot;age&quot; &quot;height&quot; ## [5] &quot;weight&quot; &quot;headband&quot; &quot;college&quot; &quot;tattoos&quot; ## [9] &quot;tchests&quot; &quot;parrots&quot; &quot;favorite.pirate&quot; &quot;sword.type&quot; ## [13] &quot;eyepatch&quot; &quot;sword.time&quot; &quot;beard.length&quot; &quot;fav.pixar&quot; ## [17] &quot;grogg&quot; Or look at the summary summary(dataset) ## id sex age height ## Min. : 1.0 Length:1000 Min. :11.00 Min. :129.8 ## 1st Qu.: 250.8 Class :character 1st Qu.:24.00 1st Qu.:161.4 ## Median : 500.5 Mode :character Median :27.00 Median :169.9 ## Mean : 500.5 Mean :27.36 Mean :170.2 ## 3rd Qu.: 750.2 3rd Qu.:31.00 3rd Qu.:178.5 ## Max. :1000.0 Max. :46.00 Max. :209.2 ## weight headband college tattoos ## Min. : 33.00 Length:1000 Length:1000 Min. : 0.000 ## 1st Qu.: 62.08 Class :character Class :character 1st Qu.: 7.000 ## Median : 69.55 Mode :character Mode :character Median :10.000 ## Mean : 69.74 Mean : 9.429 ## 3rd Qu.: 76.90 3rd Qu.:12.000 ## Max. :105.60 Max. :19.000 ## tchests parrots favorite.pirate sword.type ## Min. : 0.00 Min. : 0.000 Length:1000 Length:1000 ## 1st Qu.: 6.00 1st Qu.: 1.000 Class :character Class :character ## Median : 15.00 Median : 2.000 Mode :character Mode :character ## Mean : 22.69 Mean : 2.819 ## 3rd Qu.: 30.00 3rd Qu.: 4.000 ## Max. :147.00 Max. :27.000 ## eyepatch sword.time beard.length fav.pixar ## Min. :0.000 Min. : 0.0000 Min. : 0.00 Length:1000 ## 1st Qu.:0.000 1st Qu.: 0.2175 1st Qu.: 0.00 Class :character ## Median :1.000 Median : 0.5850 Median : 9.00 Mode :character ## Mean :0.658 Mean : 2.5427 Mean :10.38 ## 3rd Qu.:1.000 3rd Qu.: 1.3300 3rd Qu.:20.00 ## Max. :1.000 Max. :169.8800 Max. :40.00 ## grogg ## Min. : 0.00 ## 1st Qu.: 8.00 ## Median :10.00 ## Mean :10.14 ## 3rd Qu.:12.00 ## Max. :21.00 # You can also use the skimr package and skim command here # skim(pirates) To see how many columns and rows there are, you can use the nrow() and ncol() commands # there are 1000 nrow(dataset) ## [1] 1000 ncol(dataset) ## [1] 17 1.8.2 Selecting individual values You can select rows and columns using SQUARE brackets and [rows,columns]. For example #select row 2 and columns 4,5 &amp; 6 (that&#39;s what the : does, counts from 4 to 6) dataset[2 , 4:6] ## height weight headband ## 2 209.25 105.6 yes #select rows 1,3 and 5 - plus the height and eyepatch columns # the c() command lets you choose whatever values you want and sticks (concatenates) them together dataset[c(1,3,5),c(&quot;height&quot;,&quot;eyepatch&quot;)] ## height eyepatch ## 1 173.11 1 ## 3 169.95 1 ## 5 157.85 1 # Leave the right side empty for all the columns # e.g this prints all columns of rows 2,3 and 4 dataset[2:4, ] ## id sex age height weight headband college tattoos tchests parrots ## 2 2 male 31 209.25 105.6 yes JSSFP 9 11 0 ## 3 3 male 26 169.95 77.1 yes CCCC 10 10 1 ## 4 4 female 31 144.29 58.5 no JSSFP 2 0 2 ## favorite.pirate sword.type eyepatch sword.time beard.length fav.pixar grogg ## 2 Jack Sparrow cutlass 0 1.11 21 WALL-E 9 ## 3 Jack Sparrow cutlass 1 1.44 19 Inside Out 7 ## 4 Jack Sparrow scimitar 1 36.11 2 Inside Out 9 # or all the values in a column - I leave the left side blank to select all rows # Here I assigned the answer to a new variable called b, That&#39;s why it didn&#39;t print out. b &lt;- dataset[ ,c(&quot;college&quot;,&quot;tattoos&quot;)] 1.8.3 Selecting a single column The easiest way to do this is with the $ symbol. LEARN THIS! It is incredibly useful For example, to view the age column of the pirates table, I would type pirates$age - so to make a histogram of the pirates ages, I would type hist(dataset$age,xlab=&quot;Pirates ages&quot;) 1.8.4 Selecting mulitple columns Sometimes your table is very large and you might only care about a few of the variables. So we could make a new table where we only keep the columns we care about. For example, as described in the R cheatsheet Figure 1.14: picture from the tidyverse cheatsheet Inside the tidyverse packages, there are some easy functions for selecting columns or rows. To choose specific columns, we can use the dplyr::select() function from the tidyverse package or square brackets. Whatever makes you more comfortable. Let’s select only the “age”,“height”,“sex”,“weight”,“favorite.pirate” and “beard.length” columns: # print the column names, so its easier to write the command names(dataset) ## [1] &quot;id&quot; &quot;sex&quot; &quot;age&quot; &quot;height&quot; ## [5] &quot;weight&quot; &quot;headband&quot; &quot;college&quot; &quot;tattoos&quot; ## [9] &quot;tchests&quot; &quot;parrots&quot; &quot;favorite.pirate&quot; &quot;sword.type&quot; ## [13] &quot;eyepatch&quot; &quot;sword.time&quot; &quot;beard.length&quot; &quot;fav.pixar&quot; ## [17] &quot;grogg&quot; # The &quot;traditional R way&quot; newdata &lt;- dataset[,c(&quot;age&quot;,&quot;height&quot;,&quot;sex&quot;,&quot;weight&quot;,&quot;favorite.pirate&quot;, &quot;beard.length&quot;)] # The &quot;tidyverse way&quot; # Note, i&#39;m telling it that I want select from the dplyr package because # R wanted to choose a different &quot;select&quot; function identicalnewdata &lt;- dplyr::select(as_tibble(dataset),c(age,height,weight, favorite.pirate,beard.length)) # have a look at what we did head(newdata) ## age height sex weight favorite.pirate beard.length ## 1 28 173.11 male 70.5 Jack Sparrow 16 ## 2 31 209.25 male 105.6 Jack Sparrow 21 ## 3 26 169.95 male 77.1 Jack Sparrow 19 ## 4 31 144.29 female 58.5 Jack Sparrow 2 ## 5 41 157.85 female 58.4 Hook 0 ## 6 26 190.20 male 85.4 Jack Sparrow 17 1.8.5 Selecting subsets of rows/observations Very commonly, we want to select only some rows in our table. For example, let’s say I want to look at the distribution of men’s beard lengths. This means that I only want to select the men in the pirates dataset. Again there are two ways of doing this: # e.g. select rows have the column sex set equal to male # The &quot;traditional R way&quot;. You can use either == or %in% for your selection malepirates &lt;- newdata[which(newdata$sex %in% &quot;male&quot;),] malepirates &lt;- newdata[which(newdata$sex == &quot;male&quot;),] # The &quot;tidyverse way&quot;. malepirates &lt;- dplyr::filter(newdata, sex == &quot;male&quot;) # have a look at what we did head(malepirates) ## age height sex weight favorite.pirate beard.length ## 1 28 173.11 male 70.5 Jack Sparrow 16 ## 2 31 209.25 male 105.6 Jack Sparrow 21 ## 3 26 169.95 male 77.1 Jack Sparrow 19 ## 4 26 190.20 male 85.4 Jack Sparrow 17 ## 5 30 183.52 male 84.7 Jack Sparrow 25 ## 6 20 163.65 male 70.0 Jack Sparrow 27 You can also use other symbols to select data &gt;= greater than or equal to &gt; greater than == equal to != NOT equal to &lt; less than &lt;= less than or equal to # What is the average weight of pirates who are over 150cm tall? tallpirates &lt;- dplyr::filter(dataset, height &gt; 180) mean(tallpirates$weight) ## [1] 83.9277 Or you could use multiple conditions using the &amp; symbol or | symbol # What is the average weight of MALE pirates who are over 150cm tall? tall.male.pirates &lt;- dplyr::filter(dataset, height &gt; 180 &amp; sex == &quot;male&quot;) mean(tall.male.pirates$weight) ## [1] 84.18984 # What is the maximum number of tattoos on tall pirates who have at least 1 parrot OR an eyepatch tall.parrot.eye &lt;- dplyr::filter(dataset, height &gt; 180 &amp; (parrots &gt;= 1 |eyepatch == 1 )) max(tall.parrot.eye$tattoos) ## [1] 18 1.8.6 Looking at summary tables We often want to see summary statistics of our data. We covered the summary command last week and above. But what about categorical data? Just how many pirates liked Monsters Inc.? We can look at this using the table() command. This counts the number of rows containing each category. For example, we can see here that 50 pirates out of the 1000 chose Monsters Inc as their favourite film table(dataset$fav.pixar) ## ## A Bug&#39;s Life Brave Cars Cars 2 ## 28 21 21 14 ## Finding Nemo Inside Out Monsters University Monsters, Inc. ## 162 304 94 50 ## Ratatouille The Incredibles Toy Story Toy Story 2 ## 3 66 16 25 ## Toy Story 3 Up WALL-E ## 13 118 65 We can also go beyond this. For example, let’s break it up by gender - where we can see that 2 non-binary pirates liked the film. table(dataset$fav.pixar,dataset$sex ) ## ## female male other ## A Bug&#39;s Life 21 6 1 ## Brave 7 11 3 ## Cars 10 9 2 ## Cars 2 9 5 0 ## Finding Nemo 67 85 10 ## Inside Out 139 154 11 ## Monsters University 43 48 3 ## Monsters, Inc. 20 28 2 ## Ratatouille 1 1 1 ## The Incredibles 34 28 4 ## Toy Story 6 9 1 ## Toy Story 2 10 14 1 ## Toy Story 3 7 6 0 ## Up 55 59 4 ## WALL-E 35 27 3 Going one step further, we can see that both of our two non-binary pirates who chose Monsters Inc., both use cutlasses. table(dataset$fav.pixar,dataset$sex,dataset$sword.type) ## , , = banana ## ## ## female male other ## A Bug&#39;s Life 0 0 0 ## Brave 1 1 0 ## Cars 0 0 0 ## Cars 2 1 0 0 ## Finding Nemo 3 2 1 ## Inside Out 3 10 0 ## Monsters University 4 4 1 ## Monsters, Inc. 0 3 0 ## Ratatouille 0 0 0 ## The Incredibles 0 0 0 ## Toy Story 1 1 0 ## Toy Story 2 0 0 0 ## Toy Story 3 0 0 0 ## Up 7 1 0 ## WALL-E 1 0 1 ## ## , , = cutlass ## ## ## female male other ## A Bug&#39;s Life 20 5 0 ## Brave 6 8 3 ## Cars 9 6 2 ## Cars 2 8 4 0 ## Finding Nemo 53 72 9 ## Inside Out 116 131 9 ## Monsters University 31 41 2 ## Monsters, Inc. 17 23 2 ## Ratatouille 1 1 1 ## The Incredibles 28 23 3 ## Toy Story 5 6 1 ## Toy Story 2 9 14 1 ## Toy Story 3 6 5 0 ## Up 40 54 3 ## WALL-E 30 21 1 ## ## , , = sabre ## ## ## female male other ## A Bug&#39;s Life 0 1 0 ## Brave 0 0 0 ## Cars 0 1 0 ## Cars 2 0 1 0 ## Finding Nemo 9 7 0 ## Inside Out 9 10 2 ## Monsters University 5 2 0 ## Monsters, Inc. 1 1 0 ## Ratatouille 0 0 0 ## The Incredibles 1 2 0 ## Toy Story 0 2 0 ## Toy Story 2 1 0 0 ## Toy Story 3 0 1 0 ## Up 5 1 1 ## WALL-E 0 3 1 ## ## , , = scimitar ## ## ## female male other ## A Bug&#39;s Life 1 0 1 ## Brave 0 2 0 ## Cars 1 2 0 ## Cars 2 0 0 0 ## Finding Nemo 2 4 0 ## Inside Out 11 3 0 ## Monsters University 3 1 0 ## Monsters, Inc. 2 1 0 ## Ratatouille 0 0 0 ## The Incredibles 5 3 1 ## Toy Story 0 0 0 ## Toy Story 2 0 0 0 ## Toy Story 3 1 0 0 ## Up 3 3 0 ## WALL-E 4 3 0 1.8.7 Basic plots (one variable) In general, there are two very good websites for making professional graphics in R. I regularly browse both of them along with stack overflow to generate plots to be proud of, or to ask questions like “how do I change the axis size?”. Stat-Methods https://www.statmethods.net/graphs/index.html https://www.statmethods.net/advgraphs/index.html R Graph gallery https://www.r-graph-gallery.com/index.html Histograms We discussed the basic histogram command last week: hist(variable). For example hist(malepirates$age) makes a histogram of the age column in the data.frame we created earlier on male pirates. To make it look prettier, have a look at ?hist - or this article: https://www.statmethods.net/graphs/density.html Or.. you can make any one of these histograms using the ggplot package (https://www.r-graph-gallery.com/histogram.html) Boxplots We discussed the basic boxplot command last week: boxplot(variable). For example boxplot(malepirates$age) makes a boxplot of the age column in the data.frame we created earlier on male pirates. To be more fancy: boxplot(age~sex, data=dataset, xlab=&quot;Gender&quot;,ylab=&quot;Age&quot;) #names=c(&quot;Male&quot;,&quot;Female&quot;,&quot;Non-binary&quot;)) To make it look prettier, have a look at ?boxplot - or this article: https://www.statmethods.net/graphs/boxplot.html Or.. you can make any one of these boxplots (https://www.r-graph-gallery.com/boxplot.html) QQ-Norm plots We discussed the basic qqnorm command last week: qqplot(variable). For example qqplot(malepirates$age) makes a qq-norm plot of the age column in the data.frame we created earlier on male pirates. There is a nicer version inside the ggpubr package. library(ggpubr) ggqqplot(malepirates$age,col=&quot;blue&quot;) 1.9 Tutorial 2D: Distributions and tests We have talked about several distributions and tests so far in the lab. To see the help files for most of them, see ?Distributions 1.9.1 Normal distribution To see the help file for all these: ?Normal To generate a random sample from a normal distribution: sample.normal &lt;- rnorm(n=100,mean=4,sd=2) To calculate a z score from your sample/population, you can use R as a calculator. To calculate the probability of greater/lesser than a value in a given normal distribution (e.g. you can use this as an interactive table) # probability of less than 1.7 in a normal distribution of N(4,2^2) pnorm(1.7,mean=4,sd=2,lower.tail = TRUE) ## [1] 0.1250719 # probability of greater than 1.8 in a normal distribution of N(4,2^2) 1 - pnorm(1,mean=4,sd=2,lower.tail = TRUE) ## [1] 0.9331928 # or pnorm(1,mean=4,sd=2,lower.tail = FALSE) ## [1] 0.9331928 To calculate the value for a given probability # what value is less than 60% of the data? qnorm(0.6,mean=4,sd=2,lower.tail = TRUE) ## [1] 4.506694 # what value is greater than 80% of the data? qnorm(0.8,mean=4,sd=2,lower.tail = FALSE) ## [1] 2.316758 1.9.2 Student’s t-distribution What even is this? See this nice resource: https://365datascience.com/tutorials/statistics-tutorials/students-t-distribution/ To see the help file for all these: ?TDist To calculate a t-statistic from your sample/population, you can use R as a calculator. To calculate the probability of greater/lesser than a value in a given t-distribution (e.f. you can use this as an interactive t-table) # probability of seeing less than 1.7 in a t-distribution # with 20 degrees of freedom pt(1.55,df=20,lower.tail = TRUE) ## [1] 0.9315892 To calculate the value for a given probability # what value is greater than 90% of the data in a t-distribution with df=25 qt(0.9,df=25,lower.tail = TRUE) ## [1] 1.316345 To conduct a full t-test on some data: # Conduct a one sided t-test where we think that H0: mu(age)=30 on our pirates data (e.g. H1: mu(age) != 30 ) t.test(dataset$age,mu=30,alternative=&quot;two.sided&quot;) ## ## One Sample t-test ## ## data: dataset$age ## t = -14.427, df = 999, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 30 ## 95 percent confidence interval: ## 27.00092 27.71908 ## sample estimates: ## mean of x ## 27.36 or see the detailed tutorial here: http://www.sthda.com/english/wiki/one-sample-t-test-in-r for one-sample and here for comparing two samples: http://www.sthda.com/english/wiki/unpaired-two-samples-t-test-in-r 1.9.3 Wilks Shapiro test for normality To test for normality: # Conduct a W-S test for normality on the ages of pirates shapiro.test(dataset$age) ## ## Shapiro-Wilk normality test ## ## data: dataset$age ## W = 0.99617, p-value = 0.01456 1.10 Tutorial 3A - Editing code chunks R code chunks are really useful as you can completely control the output seen in your final report. To edit these, we edit the bit INSIDE the curly brackets at the top of the code chunk. For example, Figure 1.15: Replace option1, option2 etc with the things you want the chunk to do There are a huge number of options available as described here: https://bookdown.org/yihui/rmarkdown-cookbook/chunk-options.html For now, we will focus on some basics: Q: How can I stop all the friendly “loading library” text coming up when I load a library? Ans: we include message=FALSE and warnings=FALSE at the top of the code chunk e.g. library(&quot;yarrr&quot;) Q: How can I add a figure caption? Ans. Using the fig.cap argument e.g. ```{r,fig.cap=“my caption”} Q: How can I make the code invisible? Ans: Unless specifically asked, I don’t recommend doing this in STAT-462 because we are grading you on your ability to code! But it is a useful skill. Here is how to do it for a figure insert (the most common request). I also gave my code chunk a name (chipmunk.pic), so that I can refer to it later in the text. For example, this code: Will lead to this output (the caption only shows up when you knit) (#fig:chipmunk.pic)Summer! 1.11 Tutorial 3B - Regression Models Now we will fit our first regression model. 1.11.1 “Standard” regression output The command to do this is lm() e.g. linear model. output &lt;- lm(y_column ~ x_column,data=tablename) output NOTE, THE WEIRD ~ SYMBOL. This means “depends on” and it’s how we tell R what the response variable is. E.g. y depends on x, or y=mx+c. For example for the starwars data, it would be # response = mass, predictor = height star.lm &lt;- lm(mass ~ height,data=starwars) star.lm ## ## Call: ## lm(formula = mass ~ height, data = starwars) ## ## Coefficients: ## (Intercept) height ## -13.8103 0.6386 You can also look at the summary by looking at the summary command: summary(star.lm) ## ## Call: ## lm(formula = mass ~ height, data = starwars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -61.43 -30.03 -21.13 -17.73 1260.06 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -13.8103 111.1545 -0.124 0.902 ## height 0.6386 0.6261 1.020 0.312 ## ## Residual standard error: 169.4 on 57 degrees of freedom ## (28 observations deleted due to missingness) ## Multiple R-squared: 0.01792, Adjusted R-squared: 0.0006956 ## F-statistic: 1.04 on 1 and 57 DF, p-value: 0.312 In both cases, we have an estimate of the intercept (0.6386) and of the gradient (-13.8103). We will discuss the other values in later labs/lectures. Now let’s see how to add the regression line to our scatterplot. We can do this using abline(REGRESSION_VARIABLE), where regression_variable is the name of the variable you saved the output of lm to. For example. plot(starwars$height,starwars$mass) abline(star.lm) 1.11.2 “Fancy” OLSRR regression output If you want a different way of seeing the same output, you can use the ols_regress() command inside the olsrr package. library(olsrr) star.ols.lm &lt;- ols_regress(mass ~ height,data=starwars) star.ols.lm ## Model Summary ## ------------------------------------------------------------------ ## R 0.134 RMSE 166.502 ## R-Squared 0.018 Coef. Var 174.078 ## Adj. R-Squared 0.001 MSE 28695.756 ## Pred R-Squared -0.017 MAE 46.669 ## ------------------------------------------------------------------ ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## ## ANOVA ## ---------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## ---------------------------------------------------------------------- ## Regression 29854.272 1 29854.272 1.04 0.3120 ## Residual 1635658.069 57 28695.756 ## Total 1665512.342 58 ## ---------------------------------------------------------------------- ## ## Parameter Estimates ## -------------------------------------------------------------------------------------------- ## model Beta Std. Error Std. Beta t Sig lower upper ## -------------------------------------------------------------------------------------------- ## (Intercept) -13.810 111.155 -0.124 0.902 -236.393 208.773 ## height 0.639 0.626 0.134 1.020 0.312 -0.615 1.892 ## -------------------------------------------------------------------------------------------- The ols_regress command produces beautiful output, but sometimes it doesn’t work well with other commands. So I tend to run a lm command at the same time to have both available. Sometimes, this command can produce a weird error: (#fig:olsrr.error)This is probably because you loaded the moderndive package This is probably because you loaded the moderndive package. They do not play nicely together. Save your work, restart R and do not run any line that says library(moderndive)!. 1.12 Tutorial 4A: Plotting in ggplot2 There are many tutorials out there, so I don’t want to spam you with my additional instructions. Here are my top resources for learning how to plot in ggplot2. There is a data camp course on it! https://learn.datacamp.com/courses/introduction-to-data-visualization-with-ggplot2 Modifiable examples: https://www.r-graph-gallery.com For basic plotting, I like this: http://www.sthda.com/english/wiki/ggplot2-scatter-plots-quick-start-guide-r-software-and-data-visualization Another nice tutorial to work through: https://datacarpentry.org/R-ecology-lesson/04-visualization-ggplot2.html More details: https://r4ds.had.co.nz/data-visualisation.html Every detail you could ever want. I use this as a reference. https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/ 1.13 Tutorial 4B: Identifying/removing outliers Note this tutorial is not about whether you SHOULD remove an outlier or any other value, it’s simply about the mechanics of how to do so if you choose. OK, let’s make a test dataset. I can do this using the data.frame() command. test &lt;- data.frame(VarA=c(2,5,3:7,2,6,2,10),VarB=c(1,4,3:7,5,2,5,-30)) test ## VarA VarB ## 1 2 1 ## 2 5 4 ## 3 3 3 ## 4 4 4 ## 5 5 5 ## 6 6 6 ## 7 7 7 ## 8 2 5 ## 9 6 2 ## 10 2 5 ## 11 10 -30 plot(test$VarA,test$VarB,pch=16) There is a point that looks very suspicious and we want to identify it. Method 1:Filter We can look at the plot, then use the filter command to identify it and print it out onto the screen. filter(test,VarB &lt; -20 &amp; VarA &gt;9) ## VarA VarB ## 1 10 -30 or similarly: # This version shows the row number test[(test$VarB &lt; -20)&amp;(test$VarA &gt;9),] ## VarA VarB ## 11 10 -30 We can check the outlier by recoloring it on our plot using the lines() command: (NOTE YOU NEED BOTH PLOT AND LINES IN THE SAME CODE CHUNK FOR THIS TO WORK) outlier &lt;- filter(test,VarB &lt; -20 &amp; VarA &gt;9) # Make the initial plot # Add the outlier as type=&quot;p&quot; for points. plot(test$VarA,test$VarB, pch=16) lines(outlier$VarA,outlier$VarB,col=&quot;red&quot;,type=&quot;p&quot;,pch=16) To remove it, we simply use filter the other way, e.g. use it to select everything else.. newtest &lt;- filter(test,VarB &gt; -20) plot(newtest$VarA,newtest$VarB,pch=16) Method 2: Identify Command Although R is mostly not interactive, there is one interactive tool you can use to click on a point and identify it, the identify() command. THIS ONLY WORKS IN THE CONSOLE, DON’T RUN IN A CODE CHUNK First, make sure that you can clearly see the plot quadrant of your screen (between Files &amp; Packages). In the console, plot the data e.g. run plot(test$VarA,test$VarB). You should see the plot clearly Now, in the console, type identify(test$VarA,test$VarB). When you press enter, your mouse will turn into a little cross. Click on all the points you want to identify THEN PRESS ESCAPE. The row number of the points you clicked on will appear in the plot So now we know it is row 11 that is causing the problem. We can take a look at it by typing # E.g. JUST row 11 and all the columns test[11, ] ## VarA VarB ## 11 10 -30 and we can remove it with the minus sign newtest &lt;- test[-11, ] plot(newtest$VarA,newtest$VarB,pch=16) Remember that if you do this more than once, all the row numbers will move around, so I am a fan of using identify to take a look at the data, but using filter to remove it. 1.14 Tutorial 4C Writing equations in R-Studio 1.14.1 In Markdown Double dollar signs It is relatively easy to write equations in R markdown. They use the “LaTeX” format and you put them between double dollar signs. For example, try typing $$x=2$$ on a new line of the white text area in your script ( NOT INTO A CODE CHUNK), then pressing knit. You should see: \\[x=2\\] But how do you write all the fancy equation stuff? We cheat. Create the equation you want in one of these generators, then copy the code into your script and put it between double dollar signs: https://latex.codecogs.com/eqneditor/editor.php https://www.tutorialspoint.com/latex_equation_editor.htm For example $$\\widehat{y} = b_{0}+b_{1}x$$ Shows up as \\[\\widehat{y} = b_{0}+b_{1}x\\] Single dollar signs Essentially this is the same, but the equation is part of the text and you only see the output when you press knit. For example including $x=2$ in this sentence shows \\(x=2\\) as an output. 1.14.2 Using Equatiomatic There is a really neat package called equatiomatic that we can use to directly make equations from our linear models. Tutorial here: https://cran.r-project.org/web/packages/equatiomatic/vignettes/intro-equatiomatic.html Install this package (e.g. using install.packages(\"equatiomatic\")) and load it by putting library(equatiomatic) inside your “libraries” code chunk and re-running. We can now extract an equation from any linear model variable. For example, data(starwars) mymodel &lt;- lm(mass~height,data=starwars) extract_eq(mymodel) \\[ \\operatorname{mass} = \\alpha + \\beta_{1}(\\operatorname{height}) + \\epsilon \\] If you run it in the console, the command returns the LaTeX code you need to add it into the text yourself. Let’s say our data is a sample, not the population, then we want to use lower case names for our sample statistics (b0 &amp; b1). There is no easy way to do this in the command, so I literally run the command in the console, then copy the command over to the TEXT part of the report and change the betas to ’b’s If you want to use the actual numbers, simply turn on use_coefs=TRUE. extract_eq(mymodel, use_coefs = TRUE) \\[ \\operatorname{\\widehat{mass}} = -13.81 + 0.64(\\operatorname{height}) \\] Finally, you probably don’t want the code chunk showing up, so you can use the “echo=FALSE” argument inside the {r} part to stop the code chunk from being visible (see Tutorial 3A). For example, I ran exactly the same code chunk with echo=FALSE activated and got this: \\[ \\operatorname{\\widehat{mass}} = -13.81 + 0.64(\\operatorname{height}) \\] 1.15 Tutorial 5A ANOVA There are two quick and easy ways to make an ANOVA table. Lecture 13 describes it in a lot more detail. 1.15.1 Base R Simply use the ANOVA command. For example data(starwars) mymodel &lt;- lm(mass~height,data=starwars) summary(mymodel) ## ## Call: ## lm(formula = mass ~ height, data = starwars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -61.43 -30.03 -21.13 -17.73 1260.06 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -13.8103 111.1545 -0.124 0.902 ## height 0.6386 0.6261 1.020 0.312 ## ## Residual standard error: 169.4 on 57 degrees of freedom ## (28 observations deleted due to missingness) ## Multiple R-squared: 0.01792, Adjusted R-squared: 0.0006956 ## F-statistic: 1.04 on 1 and 57 DF, p-value: 0.312 anova(mymodel) ## Analysis of Variance Table ## ## Response: mass ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## height 1 29854 29854 1.0404 0.312 ## Residuals 57 1635658 28696 1.15.2 Using OLSRR In OLSRR, it’s even easier. Our standard ols_regress() command includes ANOVA. data(starwars) mymodel &lt;- lm(mass~height,data=starwars) ols_regress(mymodel) ## Model Summary ## ------------------------------------------------------------------ ## R 0.134 RMSE 166.502 ## R-Squared 0.018 Coef. Var 174.078 ## Adj. R-Squared 0.001 MSE 28695.756 ## Pred R-Squared -0.017 MAE 46.669 ## ------------------------------------------------------------------ ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## ## ANOVA ## ---------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## ---------------------------------------------------------------------- ## Regression 29854.272 1 29854.272 1.04 0.3120 ## Residual 1635658.069 57 28695.756 ## Total 1665512.342 58 ## ---------------------------------------------------------------------- ## ## Parameter Estimates ## -------------------------------------------------------------------------------------------- ## model Beta Std. Error Std. Beta t Sig lower upper ## -------------------------------------------------------------------------------------------- ## (Intercept) -13.810 111.155 -0.124 0.902 -236.393 208.773 ## height 0.639 0.626 0.134 1.020 0.312 -0.615 1.892 ## -------------------------------------------------------------------------------------------- 1.16 Tutorial 5B Diagnostics part A We know from the lectures that there are multiple assumptions underpinning whether a regression is a good model or not.These are described in detail in: Online STAT-462 - Lesson 4 (https://online.stat.psu.edu/stat462/node/116/) Regression diagnostics are a set of tools we can use to assess these assumptions. Many regression diagnostics include plots of standardized residuals, which we will focus on today. Let’s use the example from the online textbook. Some researchers (Urbano-Marquez, et al., 1989) were interested in determining whether or not alcohol consumption was linearly related to muscle strength. The researchers measured the total lifetime consumption of alcohol (x) on a random sample of n = 50 alcoholic men. They also measured the strength (y) of the deltoid muscle in each person’s nondominant arm. testdata &lt;- read.csv(&quot;alcoholstrength.csv&quot;) mymodel &lt;- lm(strength~alcohol,data=testdata) plot(testdata$alcohol,testdata$strength,xlab=&quot;Alcohol&quot;,ylab=&quot;Strength&quot;,pch=16) abline(mymodel) 1.16.1 Residual plots It can be difficult looking at the raw data to assess whether many of the assumptions are true or not. So in this case we can look at the residuals We can access the residuals and many other model elements directly from the model names(mymodel) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; # see ?lm for more explanation testdata$strength_predicted &lt;- mymodel$fitted.values testdata$residuals &lt;- mymodel$residuals head(testdata) ## alcohol strength strength_predicted residuals ## 1 36.2 1000 1565.911 -565.9109 ## 2 39.7 1000 1462.357 -462.3570 ## 3 39.5 1080 1468.274 -388.2744 ## 4 18.2 1220 2098.474 -878.4739 ## 5 29.2 1310 1773.019 -463.0187 ## 6 32.5 1400 1675.382 -275.3822 Residuals vs fitted values The most common residual plot you will see is residuals vs predicted values for each point. You can see how to interpret it here: https://online.stat.psu.edu/stat462/node/117/ Manually this is: plot(testdata$strength_predicted,testdata$residuals) abline(h=0) #plot a horizontal line However R has many built in functions for this one: # using the olsrr package ols_plot_resid_fit(mymodel) Figure 1.16: A residual plot for data that likely passes all the tests for linear regression We want our residuals to be normally distributed, random (e.g. independent) and showing no pattern or unequal variance. Residuals vs predictor Alternatively, we can use this to manually make a plot of the residuals against the x-values (the predictor) - see here for how to interpret this and compare it to the fitted value plot https://online.stat.psu.edu/stat462/node/118/ plot(testdata$alcohol,testdata$residuals) abline(h=0) #plot a horizontal line Let’s use these to see how we can test the four assumptions underpinning linear regression (LINE). Further residual interpretation can be seen here: https://online.stat.psu.edu/stat462/node/124/ 1.16.2 Checking Linearity If you are applying a linear model to data which is non-linear, you are in all sorts of trouble. Your model is simply a bad model and you will need some sort of transformation, or simply a better model altogether. Figure 1.17: Don’t be the person who makes this linear model! ALWAYS PLOT YOUR DATA AND LOOK AT IT (even if I don’t tell you to explicitly). For more examples of why this is useful to do, see here: https://online.stat.psu.edu/stat462/node/123/ However, in real life, it can often be difficult to assess whether the fit is linear by looking at the scatterplot alone. So instead, we look at the residuals, whcih often show a pattern much more clearly. For example treadwear &lt;- read.csv(&quot;treadwear.csv&quot;) tread_model &lt;- lm(mileage~groove,data=treadwear) plot(treadwear$groove,treadwear$mileage,xlab=&quot;groove&quot;,ylab=&quot;mileage&quot;,pch=16) abline(tread_model) Figure 1.18: This looks relatively linear, but…. # using the olsrr package ols_plot_resid_fit(tread_model) Figure 1.19: A parabola would clearly fit the data better Compare this residual plot to the strength/alcohol example above in Figure 1.16. You can see in the treadwear data, the residuals depart from 0 in a very systematic manner. They are clearly positive for small x values, negative for medium x values, and positive again for large x values. Clearly, a non-linear model would better describe the relationship between the two variables. In future classes we will touch upon polynomial models that you might use to deal with this. 1.16.3 Checking Independence This is a tough one. Essentially we are asking if each row in our table is independent and representative of the population. So essentially we are looking for confounding variables. Or to put it another way, is there another variable that we need to include in our model. Let’s say our data is not independent but we notice. What happens? First off, this isn’t so terrible as whether linearity is broken. The linear relationship likely still stands between our predictor and our response variable. So for example, there really is a relationship between height and weight that’s relatively linear. Or house prices in our State College pre-regressinon example might really be taken from a normal distribution. BUT, it we had taken into account this new variable ALSO, we might have done an even better job. So for example, the relationship between height and weight is definitely affected by someone’s gender. Or in the example from Lab 3, knowing how close a house is to “downtown” is a better predictor of price than knowing how far East/West it is. Equally, the non independence of our data makes it hard for us to be sure how good our model is or for us to make predictions. For example, if we were looking at height and weight and our sample was made up only of people who identified as women, then there might be clear limitations in predicting the weight of the men’s basketball team. Because every case is different, there is no “set method” for checking independence, but there are a few common ways to check things that you are suspicious of. So the first item is to think about the problem. What might impact your response variable that you have not taken into account. Covariance matrix Looking at correlations is a quick (but often misleading) way to assess what is happening. Essentially we can look at the correlation between each column of data. The basic way to do this is very quick and easy. For example, let’s look at our Star Wars height/data. The GGally function allows us to see the histogram of each column we choose to plot - along with the correlation of any numeric pairs. Let’s look at this for a new dataset looking at how much people are tipped in New York restaurants. # Quick display of two capabilities of GGally, to assess the distribution and correlation of variables house &lt;- read_excel(&quot;Lab03_house.xlsx&quot;) # Choose column names - let&#39;s say I don&#39;t care about location colnames(house) ## [1] &quot;House.Price&quot; &quot;House.Age&quot; &quot;Distance.Station&quot; &quot;Number.Shops&quot; ## [5] &quot;Latitude&quot; &quot;Longitude&quot; # Create plot - note I have message=TRUE and warning=TRUE turned on at the top of my code chunk ggpairs(house[,c(&quot;House.Price&quot; ,&quot;House.Age&quot;,&quot;Distance.Station&quot;,&quot;Number.Shops&quot; )]) You can simply look at the correlations of any NUMERIC columns using the corrplot code, as we did in Lab 3. library(corrplot) house.subset &lt;- house[ , sapply(house,is.numeric)] corrplot(cor(house.subset),method=&quot;ellipse&quot;,type=&quot;lower&quot;) There are LOADS of other ways to run correlation plots here: https://www.r-graph-gallery.com/correlogram.html Feel free to choose a favourite. Importantly, remember back to this website - https://www.tylervigen.com/spurious-correlations. Just because another variable is correlated with our response does not mean it HAS to be in the model. It simply means that you might want to consider whether there is a reason for that correlation. Scatterplots and residual plots with a suspected confounding variable A second way we looked at in a previous lab was to add colour to our scatterplot and see if we can see any influence from a suspected confounding variable. # Create a plot p &lt;- house %&gt;% ggplot( aes(Latitude,House.Price, col= Distance.Station)) + geom_point() + theme_classic()+ scale_color_gradient(low=&quot;blue&quot;, high=&quot;red&quot;) # and actually plot it ggplotly(p) However, this can be hard to see - so instead we can plot the residuals, but have the x-axis as the the variable we think is affecting independence. For example housemodel &lt;- lm(House.Price~Latitude,data=house) house$price_predicted &lt;- housemodel$fitted.values house$residuals &lt;- housemodel$residuals plot(house$Distance.Station, house$residuals) Figure 1.20: These residuals do not look randomly distributed on the page.. This definitely doesn’t look like a random cloud! It looks like distance to the station is really impacting the linear model between house price and latitude. In this case we will use MULITPLE REGRESSION. (later in the course) If your data includes a date or time-stamp Then you have time-series data and there is an ENTIRE COURSE on modelling it (https://online.stat.psu.edu/stat510/). For now, if your data contains any time stamp or date, be very cautious. Changes are that your data contains auto-correlation e.g. the response tomorrow is likely to be affected by the response today. In the short term, if you know the order in which your data was collected, you could plot a residuals/order plot as described here: https://online.stat.psu.edu/stat462/node/121/. If your data includes a location Spatial data is often not independent due to Tobler’s Law (close together things are more similar than those further apart). For example, imagine you were looking at the correlation between money spent on alcohol and undergraduate degree score Your relationship might be different in areas with lower drinking age-limits, or in countries where drinking is outlawed. So knowing the location of your sample is important. The easiest thing we can do here to check for independence is to make a literal map of the residuals. For example, remember in lab 3 - let’s do something similar # Command from the sf library # Make a spatial version of the data using the Longitide and Latitude columns house.spatial &lt;- st_as_sf(house,coords=c(&quot;Longitude&quot;,&quot;Latitude&quot;),crs = 4326) # make interactive, for static set as &quot;plot&quot;, for interactive set as &quot;view&quot; tmap_mode(&quot;plot&quot;) # Command from the tmap library # and plot qtm(&quot;Esri.WorldTopoMap&quot;) + qtm(house.spatial, # data symbols.col=&quot;residuals&quot;, # which column for the symbols symbols.size=.4, # how big symbols.palette=&quot;RdBu&quot;, midpoint=0) # color breaks There definitely looks like the residuals are not randomly distributed in space, which makes me suspicious that there is spatial autocorrelation (e.g. correlation with itself - the location of a point impacts the residual). We can test this through a statistic called Moran’s I - which compares the residual at each point against its residuals for surrounding points. In my spatial statistics course, we spend about 3 weeks on Moran’s I, so there is a lot of nuance we are skipping over here. For more, see https://mgimond.github.io/Spatial/spatial-autocorrelation.html library(moranfast) moranfast(house$residuals, house$Longitude, house$Latitude, alternative = &quot;two.sided&quot;) ## $observed ## [1] 0.1732675 ## ## $expected ## [1] -0.002421308 ## ## $sd ## [1] 0.02267439 ## ## $p.value ## [1] 9.325873e-15 But essentially here we have a hypothesis test: H0: There is no spatial autocorrelation. The “colour”/value of each residual has complete spatial randomness. I = 0 H1: Spatial autocorrelation is present. There is a significant pattern in the “colour”/values of each point I != 0 Test - The Global Moran’s I statistic is the correlation coefficient for the relationship between a variable (like income) and itself at surrounding values. P-value - the probabiity of seeing this pattern if H0 really was true. So here in this case, the p-value is very low, confirming what we visually see in the plot above. There is evidence to reject the null hypthesis and suggest that spatial autocorrelation is present. If this is true, we can use spatial GLMs to take this into account (again, later in the course). 1.16.4 Checking Normality There are three things we might want to check around normality and regression - but essentially we are testing whether the RESIDUALS are normal around the regression line. To get the best estimates of parameters such as B0 and B1, the residuals must be normally distributed around the regression line. but this has a much smaller effect typically than non-linearity or breaking assumptions of independence. Breaking normality is only important in the calculation of p values for significance testing and confidence intervals, but this is only a problem when the sample size is small. When the sample size is largeer (&gt;200), the Central Limit Theorem ensures that the distribution of residuals will approximate normality when calculating parameters. So never throw away your data if it breaks normality. But it’s good to take a look. Let’s go back to our original test data: mymodel &lt;- lm(strength~alcohol,data=testdata) plot(testdata$alcohol,testdata$strength,xlab=&quot;Alcohol&quot;,ylab=&quot;Strength&quot;,pch=16) abline(mymodel) To test normality, we can use OLSRR to: #Create a QQ plot of the residuals ols_plot_resid_qq(mymodel) #Create a histogram of the residuals ols_plot_resid_hist(mymodel) #Run a Wilks-Shapiro test for normality ols_test_normality(mymodel) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9877 0.8767 ## Kolmogorov-Smirnov 0.0502 0.9990 ## Cramer-von Mises 4.1667 0.0000 ## Anderson-Darling 0.1699 0.9289 ## ----------------------------------------------- In this case, our residuals look very normal. Compare that to our house dataset, which suggests that the data is mildly non normal. housemodel &lt;- lm(House.Price~Latitude,data=house) plot(house$House.Price~house$Latitude,xlab=&quot;Latitude&quot;,ylab=&quot;House Price&quot;,pch=16) abline(housemodel) #Create a QQ plot of the residuals ols_plot_resid_qq(housemodel) #Create a histogram of the residuals ols_plot_resid_hist(housemodel) #Run a Wilks-Shapiro test for normality ols_test_normality(housemodel) ## Warning in ks.test(y, &quot;pnorm&quot;, mean(y), sd(y)): ties should not be present for ## the Kolmogorov-Smirnov test ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9411 0.0000 ## Kolmogorov-Smirnov 0.0776 0.0138 ## Cramer-von Mises 34.21 0.0000 ## Anderson-Darling 4.3556 0.0000 ## ----------------------------------------------- The answer to non-normal data is to use a GLM (Generalised Linear Model), which again we will get to later in the semester. Alternatively you can use your linear model, but make the significance level more rigarous (say 1% rather than 5%) to take into account the fact that the assumptions underpinning the calculations might be flawed. In terms of detailed interpretation, read this page: https://online.stat.psu.edu/stat462/node/122/ 1.16.5 Checking Equal Variance/homoskadisity The easiest thing to do here is to plot the residuals, then see if you see any variations in variance. For example in our house regression, the residuals look like this ols_plot_resid_fit(housemodel) You can clearly see here that for low values of the fitted data, there is not much variance - but there is a lot of variance between 30-50 You can also run statistical tests, explained in more detail here: https://rpubs.com/tskam/Session06 # F test - assumes residuals are independent and identically distributed (i.i.d.) ols_test_f(housemodel) ## ## F Test for Heteroskedasticity ## ----------------------------- ## Ho: Variance is homogenous ## Ha: Variance is not homogenous ## ## Variables: fitted values of House.Price ## ## Test Summary ## ------------------------- ## Num DF = 1 ## Den DF = 412 ## F = 2.483727 ## Prob &gt; F = 0.1157969 In this case, we can see that perhaps we neeed to worry less about unequal variance, There is also another test available, the Breusch Pagan Test - but this relies on the residuals being normally distributed. 1.17 Tutorial 5C Confidence and Prediction Intervals Essentially these are two different things: Confidence interval: “Error bar” on the location of the regression line at any specific value of x, or “what is the uncertainty on the population mean of y, for a specific value of x?” _Prediction interval: Predicting the range of likely y-values for a new data-point, or “if you had to put a new dot on the scatterplot, what range of y values would it fall under” These are described in detail here: https://online.stat.psu.edu/stat462/node/125/ https://online.stat.psu.edu/stat462/node/126/ https://online.stat.psu.edu/stat462/node/127/ https://online.stat.psu.edu/stat462/node/128/ In R: #For our pirate weight/height dataset lm.pirate &lt;- lm(weight~height,data=pirates) summary(lm.pirate) ## ## Call: ## lm(formula = weight ~ height, data = pirates) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.3592 -2.6564 -0.0708 2.7275 11.1451 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -68.87722 1.71250 -40.22 &lt;2e-16 *** ## height 0.81434 0.01003 81.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.928 on 998 degrees of freedom ## Multiple R-squared: 0.8684, Adjusted R-squared: 0.8683 ## F-statistic: 6587 on 1 and 998 DF, p-value: &lt; 2.2e-16 # What are the AVERAGE WEIGHTS (and uncertainty on our estimate) # of pirates whose heights are 150cm and 170cm? new.pirates &lt;- data.frame(height=c(150,170)) predict(lm.pirate,newdata=new.pirates,interval=&quot;confidence&quot;,level=0.95) ## fit lwr upr ## 1 53.27345 52.80652 53.74037 ## 2 69.56020 69.31640 69.80400 So we are 95% certain that on average, the AVERAGE weight of pirates who are 150cm tall falls between 52.8Kg and 53.74Kg # A new pirate joins and her height is 160cm. What range of weights is she likely to have? new.pirates &lt;- data.frame(height=c(160)) predict(lm.pirate,newdata=new.pirates,interval=&quot;predict&quot;,level=0.95) ## fit lwr upr ## 1 61.41682 53.70196 69.13169 Her weight is likely to be somewhere between 53Kg and 69Kg with 95% certainty given our sample. 1.18 Tutorial 6A Diagnostics part B: Outliers There are three key things to know when it comes to outliers: We use the word outlier to describe an observation (point on the scatterplot) that has a very different response from the predicted response from the model e.g. it does not follow the trend determined by the rest of the dataset. We consider it an outlier only if it is an extreme “y” value e.g. an extreme response. If a data point has an x-value that is extremely different (either too high or too low) from the rest of the data points, we call this a high leverage point. It might, or might not be an outlier. We call a data point an influential point if that data point has a considerable impact on the regression model. For instance, if the model fit changes considerably by removing a point, such data point is called an influential point. Influential points tend to be further from the mean. We know the regression lines goes through the mean of x and the mean of y, they tent to tilt like a see-saw. Both outliers and the leverage points can be influential depending on where they located in a scatterplot. We can easily see them using a fits vs residuals plot (we’ll get to this below) For more details here, see https://online.stat.psu.edu/stat501/lesson/11/11.1 1.18.1 Visual inspection The first way we can check for simple linear regression is to plot the data and take a look. Here are some examples that we can assess by eye which show the different effects. No outliers, influential or high leverage points Figure 1.21: No outliers, influential values or high leverage points In the figure above, all of the data points follow the general trend of the rest of the data, so there are no outliers (in the y direction). And, none of the data points are extreme with respect to x, so there are no high leverage points. Overall, none of the data points would appear to be influential with respect to the location of the best fitting line. e.g. if we removed any one point, the line would probably be the same. An outlier with no leverage Figure 1.22: One outlier. The red-dashed line is the model including all the points. The black-solid line is the model with the red point removed In the figure above, most of the data points follow the general trend of the rest of the data, but there is one clear outlier (one point that is unusual in the y direction). However, no point has an extreme x value, so there are no high leverage points. Overall, none of the data points would appear to be influential with respect to the location of the best fitting line. e.g.when we removed the red point,the line of best fit remains relatively stable. A high leverage point that isn’t an outlier Figure 1.23: No outliers,but one high leverage point. The red-dashed line is the model including all the points. The black-solid line is the model with the red point removed In the figure above, most of the data points follow the general trend of the rest of the data, so there are no outliers (in the y direction). But one data points is extreme with respect to x. Overall, none of the data points would appear to be influential with respect to the location of the best fitting line. e.g.when we removed the red point,the line of best fit remained relatively stable. An influential high leverage outlier Figure 1.24: One influential high leverage outlier. The red-dashed line is the model including all the points. The black-solid line is the model with the red point removed In the figure above, most of the data points follow the general trend of the rest of the data, with one clear outlier. This point also has high leverage abd appears to be very influential. e.g.when we removed the red point,the line of best fit changes hugely, Here with a simple regression, we can easily see outliers. This is much harder when we have many predictors. So as well as examining the data by eye, we can use diagnostic plots. 1.18.2 Detecting leverage Leverage (\\(h_{ii}\\)): is used for for measuring unusual observations in x-direction (high leverage point). Essentially, the leverage quantifies the influence that the observed response has on its predicted value. That is, if the leverage is small, then the observed response plays only a small role in the value of the predicted/modelled response. On the other hand, if the leverage is large, then the observed response plays a large role in the value of the predicted response. This quantity is based on The distance of \\(x_{i}\\) from the bulk of the x’s. The extent tjat the fitted regression line is attracted by the given point. The Leverage, \\(h_{ii}\\) associated with the ith datapoint is: \\[h_{ii}=\\frac{1}{n}+\\frac{\\left(x_i-\\overline{\\left(x\\right)}\\right)^2}{\\ \\left\\{\\sum_{i=1}^n\\left(x_j-\\overline{\\left\\{x\\right\\}}\\right)^2\\right\\}}\\] and is described further in https://online.stat.psu.edu/stat501/lesson/11/11.2 You don’t need to remember the equation as R does the calculation for you, but here are some important properties of the leverages: The leverage is a measure of the distance between the x value for the data point and the mean of the x values for all n data points. The leverage is a number between 0 and 1, inclusive. The sum of the equals p, the number of parameters (regression coefficients including the intercept). A rule of thumb would be to classify classify \\(x_{i}\\) as a high leverage point if \\[h_{ii}&gt;\\frac{2p}{n}\\] where p = number of regression coefficients (2 for SLR) and n = number of observations We can calculate and plot the leverage easily for each point in either the base package (hatvalues), or using OLSRR (ols_leverage). Let’s see how this works for our scenarios above. If you are coding this, you don’t necessarily need the left hand plot - but it’s helpful for me showing you what is going on. # read the data data &lt;- read.csv(&quot;neither.csv&quot;) #calculate the model model &lt;- lm(y~x,data=data) leverage &lt;- ols_leverage(model) # Set up 2 sub-plots one next to each other layout(matrix(c(1,2), 1, 2, byrow = TRUE)) # plot 1 (left hand side) plot(data$x,data$y,pch=16,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Dataset A&quot;); abline(model) # plot 2 (right hand side). Remember to choose your own ylim plot(data$x,leverage,pch=16,ylim=c(0,.6), xlab=&quot;x&quot;,ylab=&quot;y&quot;,type=&quot;h&quot;, main=&quot;Dataset A leverage&quot;); # Calculate the critical value of leverage and add to plot n=nrow(data) p=length(coefficients(model)) cutLev=2*p/n abline(h=cutLev,col=&quot;blue&quot;,lty=2) Figure 1.25: No outliers, influential values or high leverage points. Thr highest x point is around the threshold for high leverage, but nothing special here Figure 1.26: No outliers,but one high leverage point. The red-dashed line is the model including all the points. The black-solid line is the model with the red point removed An important distinction! There is such an important distinction between a data point that has high leverage and one that has high influence that it is worth saying it one more time: The leverage merely quantifies the potential for a data point to exert strong influence on the regression analysis. The leverage depends only on the predictor values. Whether the data point is influential or not also depends on the observed value of the reponse. 1.18.3 Detecting outliers In lab 5, we mentioned two measures that we use to help identify outliers. They are: Residuals Studentized residuals (or internally studentized residuals) (often called standardized residuals) First, briefly review these measures using this page: https://online.stat.psu.edu/stat501/lesson/11/11.3 OLSRR also offers several more plots and tests including : Cook’s D Bar Plot Cook’s D Chart DFBETAs Panel DFFITs Plot Studentized Residual Plot Standardized Residual Chart Studentized Residuals vs Leverage Plot Deleted Studentized Residual vs Fitted Values Plot Hadi Plot Potential Residual Plot For now, we will focus on one of the most effective ways to assess residuals, the studentized residual/fits plot. For example for our test data: # read the data data &lt;- read.csv(&quot;neither.csv&quot;) #calculate the model model &lt;- lm(y~x,data=data) leverage &lt;- ols_leverage(model) # plot 1 (left hand side) plot(data$x,data$y,pch=16,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Dataset C&quot;); abline(model) # plot 2 (right hand side). Remember to choose your own ylim ols_plot_resid_stud(model) There should be no absolute cut-off here (around 2-3 is a warning sign). Instead, take these as an opportunity to explore those points further. For example here is our plot with the residual: # read the data data1 &lt;- read.csv(&quot;outlier.csv&quot;) #calculate the model model1 &lt;- lm(y~x,data=data1) # plot 1 (left hand side) plot(data1$x,data1$y,pch=16,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Dataset B&quot;); abline(model1) # plot 2 (right hand side). Remember to choose your own ylim ols_plot_resid_stand(model1) Here the plot is telling me that it thinks row 21 of the dataset might be an outlier data1[21,] ## x y ## 21 4 40 1.18.4 Detecting influential points If a point is both an outlier AND has leverage, chances are it will be influential over the fit. OLSRR has a nice way of summarising both statistics. For example here are our four plots together: # read the data data &lt;- read.csv(&quot;neither.csv&quot;) data2 &lt;- read.csv(&quot;outlier.csv&quot;) data2b &lt;- data2[-21,] data3 &lt;- read.csv(&quot;leverage.csv&quot;) data3b &lt;- data3[-25,] data4 &lt;- read.csv(&quot;influential.csv&quot;) data4b &lt;- data4[-25,] #calculate the model model &lt;- lm(y~x,data=data) model2 &lt;- lm(y~x,data=data2) model3 &lt;- lm(y~x,data=data3) model4 &lt;- lm(y~x,data=data4) # Set up 4 sub-plots one next to each other layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE)) plot(data$x,data$y,pch=16,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Dataset A&quot;,col=&quot;black&quot;) abline(model) plot(data2$x,data2$y,pch=16,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Dataset B&quot;,col=&quot;red&quot;) lines(data2b$x,data2b$y,pch=16,type=&quot;p&quot;) abline(model2) plot(data3$x,data3$y,pch=16,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Dataset C&quot;,col=&quot;red&quot;) lines(data3b$x,data3b$y,pch=16,type=&quot;p&quot;) abline(model3) plot(data4$x,data4$y,pch=16,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Dataset D&quot;,col=&quot;red&quot;,ylim=c(0,100)) lines(data4b$x,data4b$y,pch=16,type=&quot;p&quot;) abline(model4) Figure 1.27: Our four examples # model A ols_plot_resid_lev(model) # model B ols_plot_resid_lev(model2) # model C ols_plot_resid_lev(model3) # model D ols_plot_resid_lev(model4) Cook’s Distance is another method for identifying influential points Cook’s distance, \\(D_i\\), is measured by deleting one observation at a time and each time re-fitting the regression line on the remaining observations. By comparing the results from the n observations to the results with ith observation deleted, we can get an idea about the influence from the ith observation on the fitted model. While there is no set cut-off for flagging a datapoint as an influential point, an observation with \\(D_i\\) would most likely be an influential point. Again for our four examples: # model A ols_plot_cooksd_bar(model,type=3) # model B ols_plot_cooksd_bar(model2,type=3) # model C ols_plot_cooksd_bar(model3,type=3) # model D ols_plot_cooksd_bar(model4,type=3) 1.18.5 Making a table of your residual diagnostics If your data is not huge, making a table of your residual diagnostics often makes it much easier to work out which points you are looking at. In fact we can do this very easily simply by making new columns in our table of data. Let’s try it with our influential dataset. testdata &lt;- read.csv(&quot;influential.csv&quot;) mymodel &lt;- lm(y~x,data=testdata) # Make a new column with the predicted y value testdata$y_predicted &lt;- mymodel$fitted.values # Make a new column with the raw residuals testdata$residuals_raw &lt;- mymodel$residuals # Make a new column with the standardisd/studentised residuals library(MASS) testdata$residuals_stud &lt;- studres(mymodel) # Make a new column with the leverage testdata$x_leverage &lt;- ols_leverage(mymodel) # Make a new column with the Cook&#39;s distance. OLSRR package testdata$cooks.distance &lt;- cooks.distance(mymodel) # Print out the table head(testdata) ## x y y_predicted residuals_raw residuals_stud x_leverage ## 1 7.4359 19.5433 18.86669 0.6766147 0.06528349 0.07647225 ## 2 20.2564 30.3125 27.04441 3.2680919 0.31064806 0.04447583 ## 3 14.6154 18.3894 23.44622 -5.0568228 -0.48479619 0.05476095 ## 4 9.2308 7.2356 20.01159 -12.7759856 -1.27279635 0.07013913 ## 5 34.3590 35.3125 36.03994 -0.7274351 -0.06901578 0.04484163 ## 6 40.7692 60.3125 40.12876 20.1837353 2.11410204 0.05732357 ## cooks.distance ## 1 0.0001844385 ## 2 0.0023377280 ## 3 0.0070421925 ## 4 0.0594946602 ## 5 0.0001168648 ## 6 0.1180798516 If you click on the NAME of the table (e.g. testdata in the environment tab) then it will open the table in a new tab which makes it easier to view things. Figure 1.28: How to sort the table in R-Studio. click the tiny arrows If you’re sick of looking at things in R, you can save the table to a csv file and open it in excel. If you are running your project, this should save into the folder where your project file is (you can add a different directory if required) write.csv(testdata,&quot;Output.csv&quot;,quote=FALSE) Finally, there might be too much data to look at things manually. You can now use commands like filter on your new table to find which rows are of interest # Filter rows with high cooks distance dplyr::filter(testdata, cooks.distance&gt;1) ## x y y_predicted residuals_raw residuals_stud x_leverage ## 1 101.5385 50 78.89125 -28.89125 -7.870108 0.5580226 ## cooks.distance ## 1 10.71394 If you decide to remove an outlier, you can do it in excel! (save the file as something new first, then read that back in). Or you can remove a specific row with the - symbol row_highcooks &lt;- which(testdata$cooks.distance &gt; 1) print(paste(&quot;Row number: &quot;, row_highcooks)) ## [1] &quot;Row number: 25&quot; newdata &lt;- testdata[-row_highcooks] ## BE CAREFUL. AS YOU REMOVE A DATA POINT ALL THE ROW NUMBERS WILL SHIFT AROUND, SO RUN THE WHICH COMMAND EACH TIME Or you can simply use filter the other way around, but again be careful that you only remove the data you mean to # Filter rows with high cooks distance newdata &lt;- dplyr::filter(testdata, cooks.distance &lt;= 1) 1.19 Tutorial 6B Transformations We will talk in the class (or read here! https://online.stat.psu.edu/stat501/lesson/9 ) about why to apply transformations. Here I will simply show you how. This is very similar to the method in 6a. We simply make a new column and use maths to fill in the value. For example: # read the data data &lt;- read.csv(&quot;neither.csv&quot;) head(data) ## x y ## 1 10.2564 16.4663 ## 2 3.0769 4.9279 ## 3 15.1282 26.4663 ## 4 27.4359 30.3125 ## 5 12.0513 13.7740 ## 6 23.0769 13.7740 To make a column with \\(\\frac{1}{x}\\) as the predictor, we do # Note I am trying to avoid spaces or special characters in my data data$transform_1overx &lt;- 1/data$x To make a column with \\(x^2\\) as the predictor, we do # Note I am trying to avoid spaces or special characters in my data data$transform_xsquare &lt;- data$x^2 To make a column with \\(\\log(x)\\) as the predictor, we do (this is the NATURAL LOG) # Note I am trying to avoid spaces or special characters in my data data$transform_lnx &lt;- log(data$x) To make a column with \\(\\log_{10}(x)\\) as the predictor, we do # Note I am trying to avoid spaces or special characters in my data data$transform_logx &lt;- log(data$x,base=10) To make a column with \\(\\sqrt{x}\\) as the predictor, we do # Note I am trying to avoid spaces or special characters in my data data$transform_sqrtx &lt;- sqrt(data$x) And let’s see the output head(data) ## x y transform_1overx transform_xsquare transform_lnx ## 1 10.2564 16.4663 0.09750010 105.193741 2.327902 ## 2 3.0769 4.9279 0.32500244 9.467314 1.123923 ## 3 15.1282 26.4663 0.06610172 228.862435 2.716561 ## 4 27.4359 30.3125 0.03644859 752.728609 3.311852 ## 5 12.0513 13.7740 0.08297860 145.233832 2.489173 ## 6 23.0769 13.7740 0.04333338 532.543314 3.138832 ## transform_logx transform_sqrtx ## 1 1.0109950 3.202561 ## 2 0.4881134 1.754109 ## 3 1.1797873 3.889499 ## 4 1.4383192 5.237929 ## 5 1.0810339 3.471498 ## 6 1.3631775 4.803842 We could now perform our linear regression with any of these as our predictor. For example newmodel &lt;- lm(y~transform_sqrtx,data=data) plot(data$y~data$transform_sqrtx,xlab=&quot;Sqrt(x) UNITS&quot;,ylab=&quot;y UNITS&quot;) You don’t have to make a new column in advance, you can do the maths within the regression itself: newmodel2 &lt;- lm(y~sqrt(x),data=data) # For Polynomial regression you can use the poly command e.g. X+X^2 newmodel3 &lt;- lm(y ~ poly(x,2),data=data) summary(newmodel3) ## ## Call: ## lm(formula = y ~ poly(x, 2), data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.945 -6.344 1.280 6.887 15.652 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.823 1.936 29.352 &lt; 2e-16 *** ## poly(x, 2)1 144.687 10.244 14.124 2.03e-13 *** ## poly(x, 2)2 -7.937 10.244 -0.775 0.446 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.24 on 25 degrees of freedom ## Multiple R-squared: 0.8889, Adjusted R-squared: 0.8801 ## F-statistic: 100 on 2 and 25 DF, p-value: 1.174e-12 equatiomatic::extract_eq(newmodel3) \\[ \\operatorname{y} = \\alpha + \\beta_{1}(\\operatorname{poly(x,\\ 2)}_{\\operatorname{1}}) + \\beta_{2}(\\operatorname{poly(x,\\ 2)}_{\\operatorname{2}}) + \\epsilon \\] 1.20 Tutorial 6C Basic model comparisons You are now being asked to assess two (or more) models and decide “which is best”. We will talk about this in more detail later in the semester, but for now, there are two easy ways we can compare different models. Coefficient of Determination \\(R^2\\) We could look first at the coefficient of variation for each model in the model summary. e.g. which model explains more variation in your response variable. AIC There is another measure called AIC (read more here: https://online.stat.psu.edu/stat501/lesson/10/10.5). For now, know that the lower the AIC, the “better” the model. This is a non parametric test that takes into account the number of predictors and the amount of data, so is often more robust to bad linear fits than \\(R^2\\) (which needs LINE to be true) Let’s compare two models now, using our transformed data: model1 &lt;- lm(y~x,data=data) model2 &lt;- lm(y~sqrt(x),data=data) model1summary &lt;- summary(model1) model2summary &lt;- summary(model2) # Adjusted R2 paste(&quot;Model 1:&quot;,round(model1summary$adj.r.squared,2) ) ## [1] &quot;Model 1: 0.88&quot; paste(&quot;Model 2:&quot;,round(model2summary$adj.r.squared,2) ) ## [1] &quot;Model 2: 0.87&quot; # AIC AIC(model1,model2) ## df AIC ## model1 3 213.2457 ## model2 3 216.4706 We can see here that although \\(adj-R^2\\) is higher for model 2, the AIC suggests that model 1 does a better job. YOU SHOULD NEVER USE THESE NUMBERS IN ISOLATION. Also look at the data and use your common sense. Use them as guidelines. 1.21 Tutorial 7A Covariance matrices THIS TUTORIAL SUMMARISES EARLIER MATERIAL FROM TUTORIAL 5B (included here for convenience) Looking at correlations is a quick way to assess what is happening. Essentially we can look at the correlation between each column of data. Importantly, remember back to this website - https://www.tylervigen.com/spurious-correlations. Just because another variable is correlated with our response does not mean it HAS to be in the model. It simply means that you might want to consider whether there is a reason for that correlation. There are two easy packages to look at this data. 1.21.0.0.1 GGally package The GGally function allows us to see the histogram of each column we choose to plot - along with the correlation of any numeric pairs. Let’s look at this for our house dataset from a previous lab. # Quick display of two capabilities of GGally, to assess the distribution and correlation of variables house &lt;- read_excel(&quot;Lab03_house.xlsx&quot;) # Choose column names - let&#39;s say I don&#39;t care about location colnames(house) ## [1] &quot;House.Price&quot; &quot;House.Age&quot; &quot;Distance.Station&quot; &quot;Number.Shops&quot; ## [5] &quot;Latitude&quot; &quot;Longitude&quot; # Create plot - note I have message=TRUE and warning=TRUE turned on at the top of my code chunk ggpairs(house[,c(&quot;House.Price&quot; ,&quot;House.Age&quot;,&quot;Distance.Station&quot;,&quot;Number.Shops&quot; )]) 1.21.0.0.2 Corrplot package You can simply look at the correlations of any NUMERIC columns using the corrplot code, as we did in Lab 3. library(corrplot) house.subset &lt;- house[ , sapply(house,is.numeric)] corrplot(cor(house.subset),method=&quot;ellipse&quot;,type=&quot;lower&quot;) 1.21.0.0.3 Other packages There are LOADS of other ways to run correlation plots here: https://www.r-graph-gallery.com/correlogram.html. Feel free to choose a favourite. 1.22 Tutorial 7B Mulitple Regression Models Fitting a multiple regression model is just like fitting Simple Linear Regression. We simply add in the other variables you want to look at. For example: Simple linear regression between our house price response variable and house age (predictor) is slr.model &lt;- lm(House.Price ~ House.Age,data=house) slr.model ## ## Call: ## lm(formula = House.Price ~ House.Age, data = house) ## ## Coefficients: ## (Intercept) House.Age ## 42.4347 -0.2515 equatiomatic::extract_eq(slr.model,use_coefs=TRUE) \\[ \\operatorname{\\widehat{House.Price}} = 42.43 - 0.25(\\operatorname{House.Age}) \\] Multiple linear regression between our house price response variable and all our predictors is: mlr.model &lt;- lm(House.Price ~ House.Age + Distance.Station + Number.Shops ,data=house) mlr.model ## ## Call: ## lm(formula = House.Price ~ House.Age + Distance.Station + Number.Shops, ## data = house) ## ## Coefficients: ## (Intercept) House.Age Distance.Station Number.Shops ## 42.977286 -0.252856 -0.005379 1.297442 equatiomatic::extract_eq(mlr.model,use_coefs=TRUE) \\[ \\operatorname{\\widehat{House.Price}} = 42.98 - 0.25(\\operatorname{House.Age}) - 0.01(\\operatorname{Distance.Station}) + 1.3(\\operatorname{Number.Shops}) \\] 1.23 Tutorial 7C Residuals in MLR Because all of our model assumtions centre around residuals, we can simply use the same code to assess LINE and outliers. E.g. # Raw residuals ols_plot_resid_fit(mlr.model) # Studentized residuals ols_plot_resid_stud(mlr.model) # QQ plot, histogram and normality test of residuals ols_plot_resid_qq(mlr.model) ols_plot_resid_hist(mlr.model) ols_test_normality(mlr.model) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.8786 0.0000 ## Kolmogorov-Smirnov 0.0858 0.0045 ## Cramer-von Mises 36.0211 0.0000 ## Anderson-Darling 7.4522 0.0000 ## ----------------------------------------------- # Cook&#39;s distance - you can change the thresholdfor &quot;influential&quot; using the type option # See ?ols_plot_cooksd_bar for what type=3 corresponds to ols_plot_cooksd_bar(mlr.model,type=3) # Influential points (not you can&#39;t change the threshold here for influential) ols_plot_resid_lev(mlr.model) We can see that there are significant issues with this particular fit that we would need to explore! As before, we can add all the model diagnostics to our original table, to allow us to query them more efficiently. # Make a new column with the predicted y value house$price_predicted &lt;- mlr.model$fitted.values # Make a new column with the raw residuals house$residuals_raw &lt;- mlr.model$residuals # Make a new column with the standardisd/studentised residuals house$residuals_stud &lt;- studres(mlr.model) # Make a new column with the leverage house$x_leverage &lt;- ols_leverage(mlr.model) # Make a new column with the Cook&#39;s distance. OLSRR package house$cooks.distance &lt;- cooks.distance(mlr.model) To view the table, type View(house) (or whatever the name of your table is) into the console, or click on its name in the environment tab. 1.24 Tutorial 7D Prediction and Confidence Intervals in MLR These are again very similar, but we need to add in our other predictors in the new data. Importantly when you make your “newdata” data frame, it needs to have values for all the predictor columns in your model. e.g. let’s look at the error bars on the average estimated price of a house with these parameter… newdata &lt;- data.frame(House.Age=20, Distance.Station=500, Number.Shops=3) # See ?predict.lm for more options predict(mlr.model,newdata=newdata,interval=&quot;confidence&quot;,level=0.95) ## fit lwr upr ## 1 39.12293 37.8721 40.37377 So we are 95% sure that the AVERAGE price of a house with these features in the underlying population is between 37.87 and 40.37. Unlike me, at this point you would also remember to add the units. To predict the range of prices of a house that was not in our sample, it’s again very similar but now we need a prediction interval. newdata &lt;- data.frame(House.Age=20, Distance.Station=500, Number.Shops=3) # See ?predict.lm for more options predict(mlr.model,newdata=newdata,interval=&quot;prediction&quot;,level=0.95) ## fit lwr upr ## 1 39.12293 20.89395 57.35191 So we are 95% sure that a house that is 20 years old, 500m from the station near 3 shops is between 20.89 and 57.83. "],
["lab-1.html", "Chapter 2 Lab 1 2.1 Getting started 2.2 Lab challenge 1 2.3 R coding basics 2.4 Lab Challenge 2 2.5 Markdown &amp; Exploratory Analysis 2.6 Lab Challenge 3 2.7 Submitting Lab 1 {Section.L1.7}", " Chapter 2 Lab 1 Welcome to STAT-462 labs. The aim of this week is to: Become familiar with the RStudio and RMarkdown interface Learn about projects and libraries Start using R commands (known as functions) Understand R Markdown and the process for submitting assignments Do some basic data analysis If the labs are causing major problems with your computer or your computer hardware is struggling (or you have any other software issue), Talk to Dr Greatrex. We can fix this and there are other options for “online R” that you can use. As these all have their own issues, getting it installed on your computer is likely easiest and the one I would like you to try first. In general, you can reach out to any of the teaching staff if you have any issue at all - we have likely see the errors hundreds of times before and we are happy to help. There is also a canvas discussion board for this lab which will be the fastest place to get an answer. https://psu.instructure.com/courses/2115020/discussion_topics/13706473 2.1 Getting started 2.1.1 Why is this class in R? There are many different types of software one can use to analyze spatial data. We’re going to focus on the R programming language because: It’s free and open source. It allows you to do traditional statistics, machine learning and data analysis. It’s a good introduction to programming, which is great for your resume. There are some great visualization tools you can use to allow R to make your own professional books, websites, resumes, presentations and interactive tools. We will explore many of these in the class 2.1.2 Installing R and R-studio In this class, we would like you to download and install BOTH R and R-Studio onto your own computers. You should have already completed this step for Homework 1. If you have not, see instructions here: https://psu.instructure.com/courses/2115020/assignments/12682456 FOR WINDOWS USERS. Sometimes R asks you to download something called RTools. You can do so here: https://cran.r-project.org/bin/windows/Rtools/ Follow the instructions closely and ask if you need support. 2.1.3 Open R-studio Figure 2.1: Icons of the R program and R-studio Open R-studio (NOT R!). You should be greeted by three panels: The interactive R console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help/Viewer (tabbed in lower right) If you click on the View/Panes/Pane Layout menu item, you can move these quadrants around. I tend to like the console to be top left and scripts to be top right, with the plots and environment on the bottom - but this is totally personal choice. Figure 2.2: The basic R-studio screen when you log on If you wish to learn more about what these windows do, have a look at this resource, from the Pirates Guide to R: https://bookdown.org/ndphillips/YaRrr/the-four-rstudio-windows.html If you have used R before, you might see that there are variables and plots etc already loaded. It is always good to clear these before you start a new analysis. To do this, click the little broom symbol in your environment tab. 2.1.4 Change a few settings R-studio wants to be helpful and will try to re-load exactly where you were in a project when you log back in. This can get really confusing, so we are going to turn this off. ON A MAC: Click on the R-studio menu button on the top left of the screen, then click Preferences. ON A PC: Click on Tools-&gt; Global Options -&gt; Preferences Now: UNCLICK “Restore most recently opened project at startup” Set “Save workspace to .RData on” exit to Never 2.2 Lab challenge 1 2.2.1 What are projects? One of the most useful new features in RStudio are R-Projects. We are going to use R-projects to store our lab data in. An R project is a place to store all your commands, data and output in the same place. Before R projects, we had to worry about where on your computer your data was saved in. Now, we can force R-Studio to immediately look inside your lab folder - making life much easier. Equally, projects allow you to have multiple versions of R-studio open. This is useful because say you have Lab-2 open, but want to re-open Lab-1 to check some code, clicking the “Lab 1 project file” will load a whole new version of R with Lab 1 ready to edit. It is really important that you stay organized in this course, for example making sure you know where you put your data and your labs. To encourage this, choose a place that makes sense on your computer, create a folder called STAT-462. can either do this outside R studio in your file explorer, or inside the “Files tab” in R studio. You should store all your 462 labs inside this folder Now we will create our first project (also see the pic): open R-Studio and click New Project, then click “new directory” then “new project”. Name your project Lab 1, then browse to your newly created STAT-462 folder and press open. Press OK and your project is created. You will also see that a new sub-folder has been created in your STAT-462 folder called Lab 1 Inside that is your Project file. (with a .Rproj extension) Figure 2.3: Instructions to create an R project Your output should look something like this: Figure 2.4: What you should see Equally, R should now be “looking” inside your Lab 1 folder, making it easier to find your data and output your results. Try typing this into the console (INCLUDING THE EMPTY PARANTHESES/BRACKETS) and see if it prints out the location of Lab 1 on your computer. If not, talk to an instructor. getwd() In the future, every time you want to work on Lab 1, rather than open R-studio directly, double click the R project file inside Lab 1 and you will get back to your work. If you’re having issues at this point or haven’t managed to get to this step, STOP! Ask an instructor for help. Now you have opened your project, take a screenshot of your R-studio page. It should look like Figure 2.4, e.g. with at least R version 4.0.3, with the Lab 1 project and the project stored in your STAT-462 folder. To take a screenshot on a mac, press Command-3. The screenshot will appear on your desktop To take a screenshot on a PC, press Alt + PrtScn Rename the screenshot to your “username_Lab1_Fig1”(for example for me it will be hlg5155_Lab1_Fig1), then place it in your Lab 1 sub-folder inside STAT-462. This folder was created when you made the project. You will need this later, so don’t skip this step. 2.3 R coding basics So now we FINALLY get to do some R-coding. First things first, first watch the 5 min video above for some pointers. Will will also go through this below: 2.3.0.1 Using R as a calculator Remember that the aim of R is to type in commands to get your computer to analyse data. The console (see Figure 2.2) is a space where you can type in those commands and it will directly print out the answer. You’re essentially talking to the computer. The little “&gt;” symbol in the console means that the computer is waiting for your command. Let’s start by the simplest command possible. Try typing each of the following commands into your R console and pressing Enter as you work through this. 1+1 ## [1] 2 Note that spacing does not matter: 1+1 will generate the same answer as 1 + 1. When using R as a calculator, the order of operations is the same as you would have learned back in school, so use brackets to force a different order. For example, 3 + 5 * 2 ## [1] 13 will give a different result to (3 + 5) * 2 ## [1] 16 2.3.0.2 Adding text Now for text. Can you say hello world? Figure 2.5: Your screen after running the project Nope, there is an error! To make R understand text, it is important to use quote marks. print(&quot;Hello World&quot;) ## [1] &quot;Hello World&quot; 2.3.0.3 Comparisons We can also do comparisons in R - using the special symbols TRUE or FALSE (no quote marks, they are special). Here we are asking R whether 1 is equal to 1. # note two equals signs is read as &quot;is equal to&quot; 1 == 1 ## [1] TRUE We could also have used != “Not equal to” &lt; “Less than” &lt;= \"Less than or equal to` &gt; “Greater than” &gt;= “Greater than or equal to” What if I press Enter too soon? If you type in an incomplete command, R will wait for you to complete it. For example, if you type 1 + and press enter, R will know that you need to complete the command So it will move onto the next line but the &gt; will have changed into a +, which means its waiting for you to complete the command. If you want to cancel a command you can simply hit the “Esc” key or press the little stop symbol and R studio will reset. Pressing escape isn’t only useful for killing incomplete commands: you can also use it to tell R to stop running code (for example if it’s taking much longer than you expect), or to get rid of the code you’re currently writing. 2.3.1 Variables and assignment It’s great to be able to do maths easily on the screen, but really we want to be able to save our results, or load in data so we can run more complex commands. In R, we can give our data a name e.g. we save our data as a variable. So then, instead of typing the whole command, we can simply type the variable’s name and R will recall the answer. The symbol to store data into a variable is using the assignment arrow &lt;-, which is made up of the left arrow and a dash. You can also use the equals sign, but it can cause complications later on. Try typing this command into the console: x &lt;- 1/50 Notice that pressing enter did not print a value onto your screen as it did earlier. Instead, we stored it for later in something called a variable, with the name ‘x’. So our variable x is now associated with the value 0.02, or 1/50. You can print a variable on screen by typing its name, no quotes, or by using the print command. Try printing out your variable. x ## [1] 0.02 # or print(x) ## [1] 0.02 Look for the Environment tab in one of the panes of RStudio, and you will see that ‘x’ and its value have appeared. This ‘x’ variable can be used in place of a number in any calculation that expects a number. Try typing log(x) ## [1] -3.912023 Notice also that variables can be reassigned: x &lt;- 100 print(x) ## [1] 100 x used to contain the value 0.025 and and now it has the value 100. Note, the letter x isn’t special in any way, it’s just a variable name. You can replace it with any word you like as long as it contains no spaces and doesn’t begin with a number. Different people use different conventions for long variable names, these include periods.between.words.1 underscores_between_words camelCaseToSeparateWords What you use is up to you, but be consistent. Finally, R IS CASE SENSITIVE. X and x are different variables! h &lt;- 1 H &lt;- 2 ans &lt;- h+H print(ans) ## [1] 3 Combining variables As I showed above, you can now use multiple variables together in more complex commands. For example, try these commands: #Take the variable x, add 1 then save it to a new variable called y y &lt;- x + 1 # print the multiple of 2yx onto the screen y ## [1] 101 Now you can see that there are two variables in your environment tab, x and y. Where y is the sum of the contents of x plus 1. The way R works is that first it looks for the commands on the right of the arrow. It runs all of them, calculates the result, then saves that result with the name on the left of the arrow. It does not save the command itself, just the answer. For example, in this case, R has no idea that y was created using a sum, it just knows that it is equal to the number 3. You can even use this to change your original variable . Try typing the code below in a few times into the console and see what happens. A short cut to do this is to type the commands the first time, then use the up-arrow on your keyboard to cycle back through previous commands you have typed x &lt;- x + 1 # notice how RStudio updates its description of x in the environment tab x # print the contents of &quot;x&quot; onto the screen ## [1] 101 Our variables don’t have to be numbers. They could refer to tables of data, or a spatial map, or any other complex thing. We will cover this more in future labs. 2.3.2 Functions A command is simply an action you can take - like pressing the square root button on a calculator, followed by the number you wish to take the square root of. A command is always followed by parentheses ( ), inside which you put your arguments. The power of R lies in its many thousands of these built in commands, or functions. In fact, we have already come across one - the print command. Some more examples include: plot(x=1:10,y=1:10) This will plot the numbers 1 to 10 against 1 to 10 x &lt;- nchar(\"hello\") This will count the number of letters in the word “hello” (e.g. 5), then save it as a variable called x Watch this short video to learn three important facts about functions: One example of a function is file.choose() (not how I put the parentheses in this lab book so you can see it is a command). This command will let you interactively select a file and print the address out onto the screen. Try each of these out in your console for the file.choose() command, leaving the parentheses blank. # Typing this into the console will print out the underlying code file.choose # Typing it WITH parentheses will run the command. file.choose() # Typing a ? in front will open the help file for that command ?file.choose Sometimes we need to give the command some additional information. Anything we wish to tell the command should be included inside the inside the parentheses (separated by commas). The command will literally only know about the stuff inside the parentheses. sin(1) # trigonometry functions. Apply the sine function to the number 1. ## [1] 0.841471 log(10) # natural logarithm. Take the natural logarithm of the number 10. ## [1] 2.302585 This following command will plot the number 1 to 10 against the numbers 12 to 20, along with some axis labels. When you run this, the plot will show up in the plots tab. # plot the numbers 1 to 10 against the numbers 11 to 20 plot(1:10,11:20,col=&quot;dark blue&quot;, xlab=&quot;x values&quot;,ylab=&quot;STAT-462 is the best&quot;) If you are feeling lost, https://swcarpentry.github.io/r-novice-gapminder/01-rstudio-intro/ is a really good website which goes over a lot of this in more detail. A lot of this is based on their work. 2.3.3 Packages There are now several million commands/functions available for you to use in R. To make sure your computer doesn’t get overwhelmed, it doesn’t load all of these at once. In fact many need to be downloaded from the internet. So we have R: The programming language itself Functions: Specific commands or actions written in the R language Packages: Commands are grouped into bundles/apps called packages, which we download from the internet and load every time we need them. A close analogy is your phone. There are millions of apps available from banking, to social media to camera filters. You don’t have every app in the world installed on your phone - and you don’t have every app you do download running at the same time. This is the same for R: You download and install packages from the internet that you might need. This can be done by clicking the install button in the Packages tab. Or you can use the install.packages() command. You only ever need to do this once. To actually run the commands in the package you need to load/run them - just in the way you tap an app to start it. This can be done using the library() command. 2.4 Lab Challenge 2 Now we are going to download some packages from the internet and install them. You must be connected to the internet to make this happen! In the console, type the following commands, or click the “Install” button in the packages tab (next to plots) and find the package name. If it asks if you want to install dependencies, say yes. If R gives you warnings about rtools, ignore them or follow the instructions at the top to install R-Tools. # COPY/PASTE THESE INTO YOUR CONSOLE AND LET THEM RUN. #(These are hopefully all the packages for the course, so it&#39;s a one off download) install.packages(&quot;tidyverse&quot;) # Lots of data processing commands install.packages(&quot;knitr&quot;) # Helps make good output files install.packages(&quot;rmarkdown&quot;) # Helps make good output files install.packages(&quot;lattice&quot;) # Makes nice plots install.packages(&quot;RColorBrewer&quot;) # Makes nice color-scales install.packages(&quot;ISLR&quot;) # contains a credit dataset install.packages(&quot;yarrr&quot;) # contains a toy dataset about pirates install.packages(&quot;skimr&quot;) # Summary statistics install.packages(&quot;Stat2Data&quot;) # Regression specific commands install.packages(&quot;olsrr&quot;) # Regression specific commands install.packages(&quot;nortest&quot;) # Regression specific commands install.packages(&quot;lmtest&quot;) # Regression specific commands install.packages(&quot;IMTest&quot;) # Regression specific commands install.packages(&quot;MASS&quot;) # Regression specific commands install.packages(&quot;moderndive&quot;)# Regression specific commands install.packages(&quot;corrplot&quot;) # correlation plots install.packages(&quot;ggpubr&quot;) # Nice regression plots install.packages(&quot;car&quot;) # this one sometimes has problems, don&#39;t panic if you get errors You will see a load of red text appear in the console as it tells you its very detailed status of how it’s downloading and installing. Don’t panic! It might also take several minutes to do this, longer on a bad internet connection. We are doing this as a one off at the start of the course. When you have run all the commands and waited until they have finished running (remember, when it is done, you should see the little “&gt;” symbol in the console waiting for your next command), we want to check if they have installed successfully onto your computer. To do this we are going to load them using the library command: library(&quot;tidyverse&quot;) # Lots of data processing commands library(&quot;knitr&quot;) # Helps make good output files library(&quot;rmarkdown&quot;) # Helps make good output files library(&quot;lattice&quot;) # Makes nice plots library(&quot;RColorBrewer&quot;) # Makes nice color-scales library(&quot;ISLR&quot;) # contains a credit dataset library(&quot;yarrr&quot;) # contains a toy dataset about pirates library(&quot;skimr&quot;) # Summary statistics library(&quot;Stat2Data&quot;) # Regression specific commands library(&quot;olsrr&quot;) # Regression specific commands library(&quot;nortest&quot;) # Regression specific commands library(&quot;lmtest&quot;) # Regression specific commands library(&quot;IMTest&quot;) # Regression specific commands library(&quot;MASS&quot;) # Regression specific commands library(&quot;moderndive&quot;)# Regression specific commands library(&quot;corrplot&quot;) # correlation plots library(&quot;ggpubr&quot;) # Nice regression plots library(&quot;car&quot;) # this one sometimes has problems, don&#39;t panic if you get errors If you have managed to install them successfully, often nothing happens - this is great! It means it loaded the package without errors. Sometimes, it will tell you friendly messages. For example, this is what shows up when you install the tidyverse package. It is telling you the sub-packages that it downloaded and also that some commands, like filter - now have a different meaning. E.g. originally the filter command did one thing, but now the tidyverse package has made filter do something else. Figure 2.6: Tidyverse install messages To find out if what you are seeing is a friendly message or an error, run the command again. If you run it a second time and there is no error then nothing should happen. ! IMPORTANT If you see some red errors here after multiple attempts running the commands, we will have to fix what your computer is doing together. If you see errors, then take a screenshot of the full page and talk to a TA or Dr Greatrex, or post on the Lab 1 discussion. Note - you don’t need to submit anything for lab challenge 2 - it’s here to make the rest of the semester run smoothly. Your only action is to reach out if there are errors. 2.5 Markdown &amp; Exploratory Analysis 2.5.1 What is R-markdown? So far, we’ve been typing commands into the console, but these will all be lost once you close R. Equally, it’s hard to remember or reproduce the analysis you have done . So we will now move onto writing code commands that you can save and submit. There are several types of document that you can create and save in R-Studio. A basic script (the filetype is .r). This is simply just a blank document where you can save code commands. When you “run” the commands in the script, R simply copy/pastes the commands over to the console. An R-Notebook or R-Markdown document (the filetype is .Rmd). These are documents you can use to write a report with normal text/pictures, but and also include both your R code and output. You can turn these into reports, websites, blogs, presentations or applications. For example these instructions are created using a markdown document. In this course we are going to focus on the R-Markdown format and you are going to submit your labs as html files. 2.5.2 Creating a markdown document Time to make your own. Go to the File menu on the top left, then click New File - R-Markdown. It will ask you to name and save your file. Call it STAT-462 Lab 1. Figure 2.7: You should see TWO new files appear in your lab 1 folder A file should appear on your screen - your first markdown script. Essentially, we have some space for text, some space for code, and a space at the top of the file where we can add information about themes/styles etc. Each file contains some friendly text to explain what is going on, which I have annotated here. Figure 2.8: You should see TWO new files appear in your lab 1 folder Code chunks are the grey areas and are essentially “mini consoles”. If you click on the little right triangle arrow at the top-right of the code chunk, you can run the plot command, and a plot will appear, running the code. Note, it no longer runs in the console. You can still copy things into the console, by clicking on the line you want to run and pressing Ctrl-Enter / command-Enter. Let’s try this. On line 11, delete plot(cars) and type 1+1. Now press the green arrow and the answer should appear directly under your code chunk. Now click at the end of the script, around line 20 and press the green insert button (top right in the script menu icons). Insert a new R code chunk. Inside, type 1+2 and run it. So you can see that you can have text/pictures/videos etc, and code chunks interspersed throughout it. Now press the “knit” button (right next to the save button). It will first ask you to save. Save it as STAT462_Lab1_PSU.ID e.g. STAT462_Lab1_hlg5155. Then, this should make a new file pop-up, a html pretty version of your code and output. If you go to your lab 1 folder, you will see that this has appeared next to your .Rmd file. 2.5.3 “Friendly text” Much of what you see on your screen when you open a notebook document is simply a friendly introduction to RStudio. So your notebook file is essentially a Word document with the ability to add in “mini R-consoles”, AKA your code chunks. Imagine every time you opened a new word document, it included some “friendly text” (“hi, this is a word document, this is how you do bold text, this is how you save”). This is great the first time you ever use Word, but kind of a pain after that. RStudio actually does this. Every time you open a notebook file, you actually do get this friendly text, explaining how to use a notebook file. Read it, make sure you understand what it is telling you, then delete all of it. So delete from line 6 to the end. The stuff left over is your YAML code which tells R how to create the final html file. DON’T TOUCH THAT. Figure 2.9: You should see TWO new files appear in your lab 1 folder 2.6 Lab Challenge 3 Your final challenge. In class we discussed about both numerical and graphical summaries to describe the data.You will be using the dataset mtcars available in R to make some numerical and graphical summaries. NOTE: Don’t just copy paste the codes. Take a look at the help files for each of these functions and see of they make sense to you. You can also write notes to yourself about what the code does in the white text area of the R-notebook file. Step 1: Delete all the “friendly text” in your script - you should be just left with your YAML code and a blank file (see previous section) Step 2: We’re going to work with a table of data that’s already pre-loaded into R. First, type the ?mtcars command in the console. This will bring up the help file. Read the background of the dataset and briefly summarise it in your file (in the white space). Step 3: Click on the console and type View(mtcars). This should open a new tab where you can see a table showing the data. Have a look and get comfortable. Now, create a code chunk by clicking on the Insert R-code button on the top right. Type the lines of code below into the code chunk. Then run it by pressing the little green right arrow. skim(mtcars) This command compiles the summary statistics for the table you just saw. (you applied the skim command to the mtcars table). If this doesn’t work, make sure you installed and loaded the skimr library - or try the summary() command. Below your code chunk, You should be able to see that we’re dealing with a table with 11 columns that each tell us something about cars in 1974. Now answer the following questions, making sure to label them clearly in your text as a full sentence e.g. The median weight of the cars is… Question A: What is the maximum value of the Gross horsepower of the cars? Question B: What is the median weight of the cars. If you are stuck, remember to read the help file from Step 2. Step 4: Suppose I want to compare the mileage of the vehicle for Auto and Manual transmission vehicles. We can make the comparison by making a side-by-side boxplot. Create a new code chunk. Use the code below to create the plot . # boxplot is the command. You can have a command on several lines. boxplot(mpg~am, # the miles per gallon column vs each level of transmission data=mtcars, # from the mtcars table xlab=&quot;Transmission&quot;,ylab=&quot;mpg&quot;, names=c(&quot;Auto&quot;,&quot;Manual&quot;), # make nice labels col=c(&quot;deepskyblue4&quot;,&quot;deepskyblue&quot;)) # and colours Question C: Briefly comment on what you can tell about the cars dataset from this plot. Step 5: As we discussed above, the mtcars table is loaded into R and we can look at it either by typing its name or by using commands like head() to show the first few lines. But what if we want to access a specific column? This is where the $ symbol is useful. It means, “select the column named..” For example if you type mtcars$cyl into the console, you will see that it just prints the data from the cyl column (number of cylinders) onto the screen. We could then apply a command to those numbers. For example mean(mtcars$cyl) will calculate the mean of all the values in the cyl column. Question D: Create a new code chunk. Use the mean() and sd() commands to find the actual mean and standard deviation of the mpg column. Once you have calculated these, under the code chunk, write a summary of your results as a full sentence e.g. The average miles-per-gallon of 1973-1974 car models was found to be… Step 6: We want to see if our data is normally distributed. We can do this using the histogram and normal-qq plot. The code below makes a histogram and QQNorm plot for the car horsepower. It also runs a Shapiro-Wilks test for the horsepower. hist(mtcars$hp,br=10) # The hash at the front of a line means I want R to ignore this code. It is invisible to R - a &quot;comment&quot; # These two lines were the original way to make a QQ-norm plot. Feel free to uncomment and run them # qqnorm(mtcars$mpg, pch = 1, frame = FALSE) # qqline(mtcars$mpg, col = &quot;steelblue&quot;, lwd = 2) # Instead we are going to use a nicer format from the `ggpubr` library ggqqplot(mtcars$hp,col=&quot;red&quot;) # Shapiro Wilks test shapiro.test(mtcars$hp) ## ## Shapiro-Wilk normality test ## ## data: mtcars$hp ## W = 0.93342, p-value = 0.04881 Question E: In the text, in your own words, describe what a normal QQ-plot is and how to calculate one (if you’re not sure, do a google search - there are good videos!). Question F: Run a histogram, Quantile-Quantile plot and a Shapiro-Wilks test for the MILES PER GALLON data. In the text below the plot write your conclusions about whether you are satisifed that a normal distribution can represent the miles per gallon data column and explain your reasoning. Step 7: Now we’re going to move onto some commands that allow us to calculate normal probabilities using R, rather than slowly by hand. Using the R functions pnorm() and qnorm(), we can find the cumulative probabilities and normal quantiles for any normal distribution. For example, Cat lifespans have a normal distribution with a mean of 15.4 and a standard deviation of 2.3. What’s the probability of a cat lifespan less than 14 years? ?pnorm # use the help file of ?pnorm to see what lower.tail=TRUE/FALSE does pnorm(14,mean=15.4,sd=2.4,lower.tail = TRUE) ## [1] 0.2798345 So, it’s 0.2798 - or ~28% What about the probability of a cat being older than 20? pnorm(20,mean=15.4,sd=2.4,lower.tail = FALSE) ## [1] 0.02764015 Normal quantiles go the other way around - they tell you the z score for whatever probability you require. For example, 80% of cats live to be what age? qnorm(.80,mean=15.4,sd=2.4) ## [1] 17.41989 So the the answer is 17.42 years (remember the units!) Question F: Use R to find the probability below: If X ∼ N(5,2), then find P(X ≤ 4) Then Use R to find the value of the normal quantiles: If X ∼ N(5,2), then find the value a such that P(X &lt; a) = 0.025 Question H. Records maintained by the your admin office indicate that amount of time elapsed between the submission of travel receipts and the final reimbursement of funds has approximately a normal distribution with a mean of 39 days and a standard deviation of 6 days. If you submitted your travel claim 55 days ago, what is the probability that it should have been returned by now? What might you conclude? Step 9: Question I. In the text, explain to me something you didn’t already know about R Step 10: Question J. Finally, let’s include that project screenshot to learn that element of RMarkdown Make sure the screenshot is in your Lab 1 folder. Now include a code chunk that looks similar to this, but with your user name/file name (note, it might be a .jpeg not a .png). When you run it, the screenshot should appear below the code chunk. Step 11 OPTIONAL: OPTIONAL Question K [BONUS 2%, lab capped at 100%]: Finally, if you would like your histograms and plots to look more professional, take a look at ggplot and see if you can make a histogram the mpg data using this approach instead. This is a good tutorial: https://www.r-graph-gallery.com/220-basic-ggplot2-histogram.html] 2.7 Submitting Lab 1 {Section.L1.7} Remember to save your work throughout and to spell check your writing (next to the save button). Now, press the knit button again. If you have not made any mistakes in the code then R should create a new html file which includes your answers. Figure 2.10: What you need to submit If you look at your lab 1 folder, you should see this there - complete with a very recent time-stamp. Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 1. 2.7.1 Lab 1 submission check MARKDOWN/CODE STYLE - 5 MARKS Your code and document is neat and easy to read. LOOK AT YOUR HTML FILE IN YOUR WEB-BROWSER BEFORE YOU SUBMIT. There is also a spell check next to the save button. WRITING STYLE - 5 MARKS You have written your answers below the relevant code chunk in full sentences in a way that is easy to find and grade. QUESTIONS A-D: 10 MARKS You understand basic R commands and can describe the dataset QUESTIONS E: 10 MARKS 10/10 for a detailed explanation I could provide to the class as an example (in your own words) 5/10 some attempt, but some clarification/tweaks would be needed 2/10 any attempt QUESTIONS F: 10 MARKS You can make and interpret a histogram, QQplot and a Wilks test for the MILES PER GALLON data QUESTIONS G: 10 MARKS You can calculate basic probabilities in R QUESTION H: 20 MARKS You can attempt a more lifelike question QUESTION I: 10 MARKS You engaged with the lab and can articulate your learning QUESTION J: 10 MARKS You can include a screenshot and are using projects correctly HTML FILE SUBMISSION - 5 marks RMD CODE SUBMISSION - 5 marks [100 marks total] "],
["lab-2.html", "Chapter 3 Lab 2 3.1 General information 3.2 Tutorials 3.3 Setting up your code 3.4 Markdown formatting and YAML 3.5 Confidence intervals 3.6 Hypothesis testing 3.7 Exploratory data analysis 3.8 Submitting Lab 2", " Chapter 3 Lab 2 3.1 General information Welcome to STAT-462 lab 2. The aim of this week is to: Learn how to get help Create more professional markdown files Run some more statistical tests Conduct a more free ranging exploratory data analysis The support you need to understand Lab 2 is in the Tutorials - READ THE TUTORIALS 2A,2B,2C,2D. General comments You do not have to submit things you try from the tutorials, you will just be graded on the lab 2 assignment. Future labs will be less prescriptive about the formatting - but this is a detailed tutorial so that you learn how to set it all up well. Note, your answers should be written up in full sentences. If running the labs is causing major problems for your computer or you have any other computer issue, talk to Dr Greatrex. We can fix this and there are other options for you to access R online. In general, please reach out to Dr Greatrex if you have any issue at all - we have likely see the errors hundreds of times before and we are happy to help. There is also a canvas discussion board for this lab which will be the fastest place to get an answer. https://psu.instructure.com/courses/2115020/discussion_topics/13706472 3.2 Tutorials This lab contains questions on markdown formats, hypothesis test and on exploratory data analysis. I encourage you to go and look at Tutorial 2A, 2B, 2C and 2D. All the answers are in there (for a different dataset). 3.3 Setting up your code Save all your work and if you haven’t already, create a Lab 2 folder in your STAT-462 folder Create a new R-project called Lab 2 that is linked to your Lab 2 folder (instructions in section 1.2, “Lab Challenge 1”). It will probably open a new version of R-Studio. Create a new Markdown file called “username_Lab2” (for me it will be hlg5155_Lab2) Remove the “friendly text” (see Tutorial 1E, section 1.5.3 if you have no idea what I mean) 3.4 Markdown formatting and YAML Read through tutorial 2B Follow the instructions to edit your YAML code to look like my example in Section 1.7.1. Choose a new theme for your lab script! Section 1.7.1 includes a link to different themes. Leave a blank line under your YAML code, then create a level-1 Heading called “Introduction”. Save and make sure that it will preview/knit. Leave a blank line under that then write something new you learned about R or statistics this week. Use bold and italic fonts in your answers. Make a new code chunk. Inside here, you can add all the libraries you need (if you do not have them, you can install them using the tutorial from last week). Start by entering these, but if you need any more packages you can add them here and rerun the code chunk. Run the code chunk a few times to make sure they load with no errors. # Load libraries library(tidyverse) library(dplyr) library(ggpubr) library(ISLR) 3.5 Confidence intervals Tutorial 2D will be helpful here &amp; your lecture notes. Along with the tutorial, this is a good example using R to calculate confidence intervals: https://www.cyclismo.org/tutorial/R/confidence.html Leave a blank line after the code chunk, then make a new level-1 heading called “Confidence Intervals”. Leave a blank space (I’ll stop now but you get the idea), and make a Level 2 heading called “Question A”. Answer question A below, making sure to use full sentences in your conclusions. If it helps to make your report easier to read, feel free to include the question text. Question A: A sample of 36 obese rock-hopper penguins in a zoo were put on a special diet for a year. The average weight loss was 11oz and the standard deviation of the weight loss was 19oz. (note, that a positive weight loss implies reduced weight over time). Either by hand or in R, calculate the 99% confidence interval for the true mean weight reduction. Make sure to show your workings or R code. Make a Level-2 heading called Question B and answer question B below. Question B: Based on the interval you calculate above, do you have sufficient evidence at your 99% level of significance to believe that the weight-loss programme is working and the penguins are losing weight? The average penguin actually weighs about 3Kg. Is this diet something you would recommend for meaningful weight loss? 3.6 Hypothesis testing Tutorial 2D will be helpful here along with your lecture notes. Make a new level-1 heading called Hypothesis Testing, then make a Level 2 heading called Question C - and answer: Question C: Tests are being carried out on a new drug designed to relieve the symptoms of the flu, specifically on the number of hours people can sleep. The new drug is given in tablet form one evening to a random sample of 16 people who have colds. The number of hours they sleep may be assumed to be Normally distributed and is recorded below. There is also a very large control group of people who have colds but are not given the drug. The mean number of hours they sleep is 6.6 hrs. ## [1] Hours slept by people given the new drug ## [1] 2.3 ## [1] 5 ## [1] 6 ## [1] 6.4 ## [1] 6.7 ## [1] 6.7 ## [1] 6.9 ## [1] 7.2 ## [1] 7.2 ## [1] 7.4 ## [1] 7.6 ## [1] 7.8 ## [1] 7.9 ## [1] 8.1 ## [1] 8.1 ## [1] 9.7 You can enter the sleep data into R using this code. sleep &lt;- c(8.1,6.7,2.3,7.2,8.1,9.7,6.0,7.4,6.4,6.9,5.0,7.8,6.7,7.2,7.6,7.9) By hand, carry out a hypothesis test at the 1% significance level that the drug has any impact on the length of time people sleep. You can use R as a calculator to get things like the mean. Make sure to include: Your H0 and H1 The critical threshold Whether it is one sided or two sided Whether you choose to use the normal or t-distribution A diagram of the distribution split into the acceptance and critical/rejection zones The calculated test statistic and your conclusions Include a screenshot of your [neat] workings in this report. If you can’t remember how to do this, see step 10 in section 2.6 (Lab-1 Challege 3) Make a Level 2 heading called Question D and answer question B below Question D: Use R and the t.test command to calculate the t-test for the data above. Comment on whether your two results agree (e.g. did you make a mistake anywhere). 3.7 Exploratory data analysis Tutorial 2C will be useful here. Make a new level-1 heading called Exploratory Data Analysis. You are now going to explore the Credit dataset, which is a simulated dataset used for educational purposes. To start out, make a new code chunk and include following commands. dataset &lt;- ISLR::Credit ?Credit Question E: Use &amp; interpret the output of R to describe the dataset to me. How much data is there and what does the dataset show? What variable names are there? Note, the help file is incorrect for this dataset. There are not 10,000 rows of data so you’ll need to work out how big the dataset really is. Question F: Create a table of the number of students vs not students (e.g. a table of the Student column - Section 2.3.6) **. Question G: Choose a single numeric variable/column in the dataset (your choice). Tell me its summary statistics &amp; units and include a professional looking plot of its distribution. Via this and via a Wilks Shapiro test (including your H0,H1, conclusions etc) assess whether that variable is normally distributed. Question H (OPTIONAL):[BONUS 2%, lab capped at 100%] Subset/filter the data to select only rows containing students. (Section 2.3.5). Find how the mean of your variable has changed. 3.8 Submitting Lab 2 Remember to save your work throughout and to spell check your writing (next to the save button). Now, press the knit button again. If you have not made any mistakes in the code then R should create a new html file which includes your answers. This can be found in your Lab 2 folder (see Section 1.7 for where) Check your html is complete by double clicking on to open it in your web-browser. Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 2. 3.8.1 Lab 2 submission check HTML FILE SUBMISSION - 5 marks RMD CODE SUBMISSION - 5 marks MARKDOWN/CODE STYLE - 5 MARKS LOOK AT YOUR HTML FILE IN YOUR WEB-BROWSER BEFORE YOU SUBMIT. You included the headings/sub-headings as requested. Your code and document is neat and easy to read. There is also a spell check next to the save button. WRITING STYLE - 5 MARKS You have written your answers below the relevant code chunk in full sentences in a way that is easy to find and grade. You included the bold/italic R fact. QUESTION A - 10 MARKS Penguin confidence interval You answered the questions correctly either by hand or in R, but you also showed your workings/code QUESTION B - 10 MARKS Penguin confidence interval interpretation You interpreted the questions correctly QUESTION C - 20 MARKS You accurately conducted the hypothesis test with all the requested steps included and managed to upload the screenshot. QUESTION D - 5 MARKS You reproduced your t-statistic, p-value &amp; conclusion in R. QUESTION E - 5 MARKS You described the data in a way that someone who had never seen the data before would be able to understand what it shows and how big it is QUESTION F - 5 MARKS You created the summary table and interpreted the output QUESTION G - 20 MARKS You summarised a variable of your choice and included all the requested statistics/plots. [100 marks total] "],
["lab-3.html", "Chapter 4 Lab 3 4.1 Lab 3 General information 4.2 Installing new packages 4.3 Lab 3 Setup &amp; Markdown 4.4 Regression basics 4.5 Taiwan housing challenge. 4.6 Submitting Lab 3", " Chapter 4 Lab 3 4.1 Lab 3 General information Welcome to STAT-462 lab 3. The aim of this week is to: A little more markdown (code chunks) Read a variety of data into R-Studio Make some scatter plots Explore correlation plots and statistics General comments You do not have to submit things you try from the tutorials, you will just be graded on the lab 3 assignment. There is also a canvas discussion board for this lab which will be the fastest place to get an answer. https://psu.instructure.com/courses/2115020/discussion_topics/13706470 Lab 3 Tutorials This lab contains questions on markdown formats, hypothesis test and on exploratory data analysis. I encourage you to go and look at the tutorials as needed. All the answers are in there (for a different dataset). 4.2 Installing new packages Please install the tmap, sf, plotly and readXl packages. You can either use the install.packages() command in the console, OR Click on the packages tab, between plots and help on the bottom quadrant of your screen. The list of packages you see are the ones currently downloaded on your computer, so CLICK THE INSTALL BUTTON. Start typing the package name and click install. Figure 4.1: How to manually install a package 4.3 Lab 3 Setup &amp; Markdown Save all your work and if you haven’t already, create a Lab 3 folder in your STAT-462 folder Create a new R-project called Lab 3 that is linked to your Lab 3 folder (instructions in section 1.2, “Lab Challenge 1”). It will probably open a new version of R-Studio. Create a new Markdown file called “username_Lab3” (for me it will be hlg5155_Lab3) Remove the “friendly text” (see Tutorial 1E, section 1.5.3 if you have no idea what I mean) Follow the instructions to edit your YAML code to look like my example in Section 1.7.1. (tutorial 2B). Choose any theme that you like. Leave a blank line under your YAML code, then create a level-1 Heading called “Markdown”. Save and make sure that it will preview/knit. Make a new code chunk. Inside here, you can add all the libraries you need (if you do not have them, you can install them using the tutorial from last week). Start by entering these, but if you need any more packages you can add them here and rerun the code chunk. Run the code chunk a few times to make sure they load with no errors. # Load libraries library(tidyverse) library(dplyr) library(ggpubr) library(Stat2Data) library(corrplot) library(olsrr) library(sf) # you have to have installed this first.. library(tmap) # you have to have installed this first.. library(readxl) # you have to have installed this first.. library(plotly) # you have to have installed this first.. Edit the “code chunk code” at the top of the code chunk so that the code is run and it is shown, but none of the warnings or messages show up (e.g. none of the the friendly text is shown). Choose any picture/screenshot that you like (of anything at all). Add this into your report, but edit the “code-chunk code” so that I see the image in your report, but I do not see the code you used to create the output. Add a caption to tell me about the picture you choose using the fig.cap argument (Tutorial 3A) 4.4 Regression basics Leave a blank line under your YAML code, then create a level-1 Heading called “Regression basics”. Save and make sure that it will preview/knit. For the questions below, using sub-headings for each question makes it 100 times easier to grade (please do!) First use the code below to load the dataset TextPrices from the Stat2Data package. Remember that you will have to load the package by running the “library” code chunk before anything will work. # This will only work after running library(Stat2Data) from your 1st code chunk data(TextPrices) View(TextPrices) # opens data in a new tab Describe the dataset, using the commands you learned in Labs 1 and 2, along with the help file: (?TextPrices). How much data is there? What are the columns describing? Summarise each variable Decide on a response variable for this dataset and justify your decision Decide on a predictor variable for this dataset and justify your decision Create a professional scatterplot. We can create a basic scatterplot using: plot(TextPrices$Pages,TextPrices$Price) Make your plot look more professional than this (do not use ggplot2, this is to teach “base R”). Give the plot better axis titles, change the point type and color and add anything else you would like. These tutorials should help you answer this question - https://www.r-graph-gallery.com/13-scatter-plot.html - https://www.datanovia.com/en/blog/pch-in-r-best-tips/ In the text below the code chunk, fully describe the scatterplot and study in the way I discussed in Lecture 7. Now use Tutorial 3B to make a regression model with Price as your response variable (no matter what you answered earlier). Write the equation for the regression model (e.g \\(\\widehat{y} = b_{0}+b_{1}x\\) , remembering to reference units with your answer). According to your model, how much does the price go up for every 50 pages added to the textbook? Make a new code chunk, then copy/paste your scatterplot code. Add the regression model (e.g. the line of best fit), using Tutorial 3B to help. 4.5 Taiwan housing challenge. Create a new Level 1 heading called House Prices. Next month, your friend is moving to Sindian Dist., in New Taipei City, Taiwan. They want to buy a house and have asked you to figure out what most impacts house price. Download the “Lab03_house.xlsx” dataset from the Lab page on canvas and put it into your Lab 3 folder. Use the read_excel() command to read it in and save it to a variable called house: # This only works if you are running your project # If it can&#39;t find the file, use file.choose() to locate it, as described in homework 3 # Then add in the full location rather than just the file name. house &lt;- read_excel(&quot;Lab03_house.xlsx&quot;) Explore the dataset (using summaries etc (lab 1 &amp; 2), and by reading more about the data here: https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set). Describe the dataset to your friend. What columns does the dataset contains and how much data there is? Are there any limitations using this data? Use R to explore the summary statistics and distribution of the House.Price column. What range of costs are “most” of the prices between? (say 68% or the interquartile range..) Is the house price data normally distributed? You have a been told that houses might be more expensive in the North. _ Choose your response and your predictor Create a scatterplot to assess this (hint: Latitude is the North/South coordinate). Describe the scatterplot fully as discussed in Lecture 7. Create a Simple Linear Regression model to assess the issue Plot the abline, line of best fit onto a new scatterplot Why might this be misleading as an analysis? or why might this model be flawed? What confounding variable could there be? To help you answer this question, try running this code to further explore the data on a map. # Command from the sf library # Make a spatial version of the data using the Longitide and Latitude columns house.spatial &lt;- st_as_sf(house,coords=c(&quot;Longitude&quot;,&quot;Latitude&quot;),crs = 4326) # make interactive, for static set as &quot;plot&quot; tmap_mode(&quot;view&quot;) # Command from the tmap library # and plot tm_basemap(&quot;Esri.WorldTopoMap&quot;) + qtm(house.spatial, # data symbols.col=&quot;House.Price&quot;, # which column for the symbols symbols.alpha=0.9, # transparency symbols.size=.2, # how big symbols.palette=&quot;Spectral&quot;, #colors from https://colorbrewer2.org symbols.style=&quot;fisher&quot;) # color breaks What other confounding variables are there? Is there a variable that is more important than latitude in predicting house prices? Provide evidence to justify your answer. To help you answer this data and answer this question, some useful code includes: [A] corrplot(). A quick look at the correlation coefficient between all the variables. We will discuss it more next week. corrplot(cor(house),method=&quot;number&quot;,type=&quot;lower&quot;) [B] If you then want to look at three variables together, you can use an interactive plot e.g you can use this code and change the response, y (currently house price), the predictor x (currently latitude) and the colour (currently distance from the metro station). Have a play and see what variables stand out. # Create a plot p &lt;- house %&gt;% ggplot( aes(Latitude,House.Price, col= House.Age)) + geom_point() + theme_classic()+ scale_color_gradient(low=&quot;blue&quot;, high=&quot;red&quot;) # and actually plot it ggplotly(p) You can also use standard scatterplots and regression models if that is easier. Or even excel and screenshots embedded in your report if it is all going wrong! Your friend forgot to tell you that they love shopping. They only want to live in a house that is close to at least 7 shops, but are worried that they might end up paying too much. Filter the data so that it only includes 7 or more nearby shops e.g you’re subsetting by the number of shops column (Tutorial 2C, section 1.8.6). Save the output to a new variable called house.gt7shop. Run a t.test to find out if your house.gt7shop data has a mean that is significantly higher than the mean of the house prices over the whole region (Tutorial 2D, 1.9.2). Remember to write up your hypotheses, the results and the conclusion in plain English that your friend would understand. [OPTIONAL BONUS 2%] How much does the house price go up for every 1 KILOMETER that you travel away from a metro station? Hint(remember the lm command..) 4.6 Submitting Lab 3 Remember to save your work throughout and to spell check your writing (next to the save button). Now, press the knit button again. If you have not made any mistakes in the code then R should create a new html file which includes your answers. This can be found in your Lab 3 folder and have a .html ending. Check your html is complete by double clicking on to open it in your web-browser. Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 3. (see the end of lab 1 for a screenshot) 4.6.1 Lab 3 submission check HTML FILE SUBMISSION - 5 marks RMD CODE SUBMISSION - 5 marks MARKDOWN/CODE STYLE - 5 MARKS LOOK AT YOUR HTML FILE IN YOUR WEB-BROWSER BEFORE YOU SUBMIT. You included the headings/sub-headings as requested. Your code and document is neat and easy to read. There is also a spell check next to the save button. WRITING STYLE - 5 marks You have written your answers below the relevant code chunk in full sentences in a way that is easy to find and grade. You included a figure caption (not just writing a sentence under your plot) REGRESSION BASICS Describe the dataset, &amp; choose/justify response &amp; predictors - 5 marks Create a professional scatterplot including all requested elements - 5 marks Fully describe the scatterplot in clear langauge including all requested elements - 10 marks Correctly create the regression model, interpret the regression equation and add the line of best fit - 15 marks TAIWAN DATA Describe the dataset - 5 marks Describe house price column - 5 marks Correctly follow regression analysis including all requested elements - 10 marks Comment on confounding variables &amp; why this might be misleading - 10 marks Provide evidence to show a better variable that predicts house price - 10 marks T.Test question - 5 marks [100 marks total] "],
["lab-4.html", "Chapter 5 Lab 4 5.1 Lab 4 General information 5.2 Lab 4 Setup &amp; Markdown 5.3 Public safety spending 5.4 Submitting Lab 4", " Chapter 5 Lab 4 5.1 Lab 4 General information Welcome to STAT-462 lab 4. The aim of this week is to: Finalise the basic R markdown tools (ggplot2, equations) Interpret some regression output Identify and remove outliers. General comments You do not have to submit things you try from the tutorials, you will just be graded on the lab4 assignment. There is also a canvas discussion board for this lab which will be the fastest place to get an answer. https://psu.instructure.com/courses/2115020/discussion_topics/13706463 Lab 4 Tutorials There are three new tutorials, 4A - 4C. Tutorial 4A covers how to plot things in ggplot2. Tutorial 4B covers finding, selecting and potentially removing outliers. Tutorial 4C covers including equations. 5.2 Lab 4 Setup &amp; Markdown Save all your work and if you haven’t already, create a Lab 4 folder in your STAT-462 folder Create a new R-project called Lab 4 that is linked to your Lab 4 folder (instructions in Tutorial 1B). It will probably open a new version of R-Studio. Create a new Markdown file called “username_Lab4” (for me it will be hlg5155_Lab3) Remove the “friendly text” (see Tutorial 1E if you have no idea what I mean) Follow the instructions to edit your YAML code to look like my example in Tutorial 2B. Choose any theme that you like. Leave a blank line under your YAML code, then create a level-1 Heading called “Markdown”. Save and make sure that it will preview/knit. Make a new code chunk. Inside here, you can add all the libraries you need (if you do not have them, you can install them using the tutorial from last week). Start by entering these, but if you need any more packages you can add them here and rerun the code chunk. Run the code chunk a few times to make sure they load with no errors. # Load libraries library(tidyverse) library(dplyr) library(ggpubr) library(Stat2Data) library(corrplot) library(olsrr) library(sf) library(tmap) library(readxl) library(plotly) Edit the “code chunk code” at the top of the code chunk so that the code is run and it is shown, but none of the warnings or messages show up (e.g. none of the the friendly text is shown). Throughout your lab, please make sure that you use headings and sub-headings to make your lab easier to follow and grade. 5.3 Public safety spending Suburban towns often spend a large fraction of their municipal budgets on public safety services (police, fire, and ambulance). A taxpayers’ group felt that tiny towns were likely to spend large amounts per person because they have such small financial bases. The group obtained data on the per-capita spending on public safety for 29 suburban towns in a metropolitan area, as well as the population of each town in units of 10,000 people. The data are given in the file expenditure.xslx. Download this from the lab page and put it into your lab 4 folder. Checking folder names One of the most frustrating things in R is when the headings of your data tables have spaces, special characters or anything else that is difficult to type. It makes it very hard to refer to a column/variable by name, so it’s always something we want to check for and fix. We can check this without doing anything fancy in R (you can if you want, it’s the names() command). Instead, simply open expenditure.xlsx in Excel and take a look!. Rename the column titles so that any given column name contains no spaces &amp; check you are happy with the data. Save and close. [THIS IS A TOP HINT FOR WORKING WITH YOUR OWN DATA - I check every file that’s small enough to be opened into excel]. Reading in the data spending &lt;- read_excel(&quot;expenditure.xlsx&quot;) [Step 5]. The paste() command allows you to stick together R output like text or the output of code. For example, rather than just printing out the number of rows, I can put the output into a sentence: ## So this command says # [1] I am going to print something # [2] The thing I am going to print is the output of the paste command # [3] The things I am pasting together are the sentence &quot;Number of Rows:&quot; and the output of the # nrow(mlb) command # [4] The output of the nrow(mlb) command is 150 e.g. there are 150 rows print( paste(&quot;Number of Rows:&quot;,nrow(spending)) ) ## [1] &quot;Number of Rows: 29&quot; Now it’s your turn. Create a sentence which says “Number of Columns:” and then tells me how many columns the data has. Hint, see eariler labs, use google or read ?base::nrow for to hunt for the command itself. Analysis Question 1: Use the tools you learned in previous labs to explore the data. Identify the explanatory and response variable in the problem. Question 2: If the taxpayer’s group is correct, what sign should the slope of the regression model have? Question 3: Use Tutorial 3B to fit a regession model to the data and save it as a variable called model1. Examine the coefficients and the summary of the model fit. Does the slope in the output confirms the opinion of the community group? Explain. Question 4: Last week we used “base R code” to visualise data (eg commands like plot that just come with the R installation). We are now going to use a specifics graphics package from the tidyverse, ggplot2, to create a scatterplot of the data. Use Tutorial 4A to create a scatterplot of the data using ggplot2 and to add a regression line. http://www.sthda.com/english/wiki/ggplot2-scatter-plots-quick-start-guide-r-software-and-data-visualization [recommended] https://www.r-graph-gallery.com/50-51-52-scatter-plot-with-ggplot2.html https://r4ds.had.co.nz/data-visualisation.html You can make it as fancy as you like Question 5: Does the scatter plot in part (d) suggest that the regression line estimated in part (c) is misleading? Explain. Question 6: Use Tutorial 4B to find and remove the unusual point from the dataset. (For a full study, we will be much more careful about whether to remove an outlier or not as discussed later in the semester) Repeat the linear regression and scatterplot with the new data and save it as a variable called model2. Explain how this has changed your assessment of the relationship between the variables. Question 7: Normally, to calculate the correlation coefficient between two variables, we use the cor() command or we could look at the output from ols_regress(). Let’s imagine that these have mysteriously broken From the command summary(model2), how could you quickly calculate the correlation coefficient? What is it? Question 8: We often want to write the equations for our regression lines more professionally. Use Tutorial 4C to create a professional equation showing the linear model, hiding any code chunk you (might) have used to do it. Make sure to define any terms &amp; units you might have used. Question 9: Interpret the slope of the regression line in part 9 in the context of the problem. Question 10: What is the Mean Squared Error of the new model? Why is this not a good measure to compare two models with? Question 11: Test if the slope is significantly differnet to 1. Show all your workings and ideally use the equation editor for any equations. Question 12 [OPTIONAL BONUS 2%] : Bonus. We are starting an interesting discussion about degrees of freedom. - Read this blog article: https://statisticsbyjim.com/hypothesis-testing/degrees-freedom-statistics/ - and this summary of Monday’s class. https://online.stat.psu.edu/stat501/lesson/1/1.4 Now go to this discussion board - https://psu.instructure.com/courses/2115020/discussion_topics/13706478 and try to either explain what you have learned or try to clarify some of the confusion that many many students feel when they discuss degrees of freedom. You get the 2% for a meaningful comment (more than 1 sentence). Essentially, I think you might be able to find better words than I can! 5.4 Submitting Lab 4 Remember to save your work throughout and to spell check your writing (next to the save button). Now, press the knit button again. If you have not made any mistakes in the code then R should create a new html file which includes your answers. This can be found in your Lab 4 folder and have a .html ending. Check your html is complete by double clicking on to open it in your web-browser. Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 4. (see the end of lab 1 for a screenshot) 5.4.1 Lab 4 submission check HTML FILE SUBMISSION - 5 marks RMD CODE SUBMISSION - 5 marks MARKDOWN/CODE STYLE - 5 MARKS LOOK AT YOUR HTML FILE IN YOUR WEB-BROWSER BEFORE YOU SUBMIT. You included the headings/sub-headings as requested. Your code and document is neat and easy to read. There is also a spell check next to the save button. WRITING STYLE - 5 marks You have written your answers below the relevant code chunk in full sentences in a way that is easy to find and grade. You correctly used print/paste QUESTIONS 1-3 - 15 marks You have clearly explored the data and understand its properties and what it is showing. You have correctly identified the explanatory &amp; response variables &amp; the sign that the slope should have. You have correctly fitted an R model to the data QUESTIONS 4 - 10 marks You have used ggplot2 to make a professional looking plot of the data. QUESTIONS 5&amp;6 - 10 marks You have correctly identified the unusual feature and successfully removed the point from the dataset. QUESTIONS 7 - 5 marks You have calculated the correlation coefficient AND SHOWED HOW YOU DID IT. QUESTIONS 8&amp;9 - 10 marks You have made a professional looking equation using an equation editor in Tutorial 4C. You have defined all terms and units. You have interpreted the slope within the context of the graph. QUESTION 10 - 10 marks You have found the Mean Squared Error and explained why it is not great for comparing models. QUESTION 11 - 20 marks You have conducted a full hypothesis test showing your workings. QUESTION 12 - 2 marks BONUS - meaningful comment on degrees of freedom, [100 marks total] "],
["lab-5.html", "Chapter 6 Lab 5 6.1 Lab 5 General information 6.2 Lab 5 Setup 6.3 Advertising challenge 6.4 Submitting Lab 5", " Chapter 6 Lab 5 6.1 Lab 5 General information Welcome to STAT-462 lab 5. The aim of this week is to: Solidify your knowledge of regression with less handholding ANOVA and confidence/prediction intervals. Examining linear regression assumptions General comments You do not have to submit things you try from the tutorials, you will just be graded on the lab 5 assignment. There is also a canvas discussion board for this lab which will be the fastest place to get an answer. https://psu.instructure.com/courses/2115020/discussion_topics/13706462 Lab 5 Tutorials It is worth reading the Lab 5 tutorials. Tutorial 5A) discusses ANOVA. Tutorial 5B) goes into detail on how to check model regression assumptions. Tutorial 5C) discusses confidence and prediction intervals. 6.2 Lab 5 Setup Save all your work and if you haven’t already, create a Lab 5 folder in your STAT-462 folder Create a new R-project called Lab 5 that is linked to your Lab 5 folder (instructions in Tutorial 1B). It will probably open a new version of R-Studio. Create a new Markdown file called “username_Lab5” (for me it will be hlg5155_Lab5) Remove the “friendly text” (see Tutorial 1E if you have no idea what I mean) Follow the instructions to edit your YAML code to look like my example in Tutorial 2B. Choose any theme that you like. Leave a blank line under your YAML code, then create a level-1 Heading called “Markdown”. Save and make sure that it will preview/knit. Make a new code chunk. Inside here, you can add all the libraries you need (if you do not have them, you can install them using the tutorial from last week). Start by entering these, but if you need any more packages you can add them here and rerun the code chunk. Run the code chunk a few times to make sure they load with no errors. # Load libraries library(tidyverse) library(dplyr) library(ggpubr) library(Stat2Data) library(corrplot) library(olsrr) library(sf) library(tmap) library(readxl) library(plotly) ## you may need additional libraries. Just add them to the list as you use them. Edit the “code chunk code” at the top of the code chunk so that the code is run and it is shown, but none of the warnings or messages show up (e.g. none of the the friendly text is shown). 6.3 Advertising challenge Houseplants are the new big thing and you are going to make the world want to buy them! You are a top advertising executive and you have been collecting data on how well your marketing campaigns have been running. You have run 200 marketing campaigns over the last few year. For each one, you recorded: how much you spent (in units of thousands of dollars) on TV, radio and newspaper adverts How many houseplants were sold (in thousands of plants). You also know the “X-Factor” of how popular that plant was at the time (percentage popularity), And typically how tall that type plant was in inches. Your job now is to explore the data work out which type of advertising campaign is the most effective out of. Model1 - Plant Sales vs Newspaper adverts Model2 - Plant Sales vs TV adverts Markdown answer format Imagine this is a real report in an advertising company. You will be graded on the professionality of your final report. In all of your answers below, I expect good formatting, appropriate units and full sentences to explain your answers. For example, please make sure that you use headings and sub-headings to make your lab easier to follow and grade. You are welcome to use any/all of the markdown features we have learned so far, for example equations, text formatting, pictures, code-chunk options or anything else that makes your report look more professional. All the methods to answer these questions are either things you have done in previous labs or they are in the Lab 5 Tutorials. Question 1: Read in data Read the data into R (hint, always look at it in excel first to see if your column names and the data make sense) Explore and describe the dataset along with the distributions of your individual predictors. Question 2: Make models Create &amp; summarise each of your your linear models, along with along with some high quality scatterplots &amp; the lines of besf fit, the equations of each model (e.g. within the context of the problem), and good explanations of what is going on. Hint, think about what I have asked you to do in past labs to answer this. Question 3: Favourite model Out of model 1 and model 2, where do you see the greatest increase in sales if you increase the advertising budget? Provide evidence to justify your answer (thinking about uncertainties on your estimate). Which model explains the most variability in the sales data? Provide evidence to justify your answer. Question 4: Peace lilies You have a new client who needs to sell 8000 peace lilies but hates newspapers. Conduct an hypothesis test to assess whether you typically sell less than 8000 plants in a situation where you spend zero-money on newspaper advertising. You are happy to be wrong one time in 25. Can you advise your client it is OK to not advertise in newspapers? Question 5: TV fears Another client is skeptical of TV. Use the ANOVA tqble output to conduct a hypothesis test to examine if there is evidence to suggest a relationship between TV advertising and plant sales at a significance of 1%. Question 6: Best day ever A new client has approached you with a brand new plant!!! (the lesser-variated-monstera-fig) This is very popular. The magazine “plants daily” rates its popularity at 90%! Thinking about the plant popularity independently of advertising type, what is the predicted range of sales for your new campaign? (at a 99% confidence level) Question 7: Testing SLR Assumptions Let’s return to your two models (e.g. Model1 and Model2), do either/both of them meet the requirements for simple linear regression? Provide evidence to support your answers (Use Tutorial 5B). Question 8 [OPTIONAL BONUS 2%] : How can multiple regression help us do a better job in answering this week’s lab? 6.4 Submitting Lab 5 Remember to save your work throughout and to spell check your writing (next to the save button). Now, press the knit button again. If you have not made any mistakes in the code then R should create a new html file which includes your answers. This can be found in your Lab 5 folder and have a .html ending. Check your html is complete by double clicking on to open it in your web-browser. Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 5. (See the end of Lab 1 for a screenshot) 6.4.1 Lab 5 submission check HTML FILE SUBMISSION - 5 marks RMD CODE SUBMISSION - 5 marks MARKDOWN/CODE/WRITING STYLE - 10 MARKS 10/10 - your report is very professional. There are tables of contents, headings/subheadings, your plots look great, you answer in full sentences and have used the spell check. You have written your answers below the relevant code chunk in full sentences in a way that is easy to find and grade. It’s clear you put thought and effort into writing a good markdown document. I could use this as a class example. You would be comfortable showing it in a job interview. 8/10 - your report is fine on the basics, but not quite as snazzy &amp; is clearly a homework. less - as your report becomes harder to read. QUESTION 1&amp;2 - 15 marks You have described the data &amp; models well, including all relevant information. Your scatterplots and model are professional and correct. QUESTION 3 - 10 marks Correct answers, correct method and it’s clear how you got there. You have written the answer up in full sentences. QUESTION 4 - 10 marks Correct answer, correct method and and it’s clear how you got there. You have written the answer up in full sentences. QUESTION 5 - 10 marks Correct answer, correct method and and it’s clear how you got there. You have written the answer up in full sentences. QUESTION 6 - 10 marks Correct answer, correct method and and it’s clear how you got there. You have written the answer up in full sentences. QUESTION 7 - 25 marks You have eloquently assessed the four aspects of inter QUESTION 8 - 2 marks BONUS (capped at 100%) Meaningful attempt at commenting (e.g. more than just a sentence) [100 marks total] "],
["lab-6.html", "Chapter 7 Lab 6 7.1 Lab 6 General information 7.2 Lab 6 Setup 7.3 Diagnostic summaries 7.4 Florida Fish Challenge 7.5 Submitting Lab 6", " Chapter 7 Lab 6 7.1 Lab 6 General information Welcome to STAT-462 lab 6. The aim of this week is to: Be able to identify, assess and as necessary remove residuals To show how transforming data can allows you to overcome broken linear regression assumptions. This lab also provides less explicit instructions, but builds heavily on previous labs. So if you are not sure what to do, see what I was asking you to do in previous homeworks or in the lecture notes. General comments You do not have to submit things you try from the tutorials, you will just be graded on the lab 6 assignment. There is also a canvas discussion board for this lab which will be the fastest place to get an answer. https://psu.instructure.com/courses/2115020/discussion_topics/13706461 Lab 6 Tutorials It is worth reading the Lab 6 tutorials. Tutorial 6A) discusses Outlier diagnostics, which link with Tutorial 5B). Tutorial 6B) discusses transformations and Tutorial 6C) discusses goodness of fit. 7.2 Lab 6 Setup Save all your work and if you haven’t already, create a Lab 6 folder in your STAT-462 folder Create a new R-project called Lab 6 that is linked to your Lab 6 folder (instructions in Tutorial 1B). It will probably open a new version of R-Studio. Create a new Markdown file called “username_Lab6” (for me it will be hlg5155_Lab6) Remove the “friendly text” (see Tutorial 1E if you have no idea what I mean) Follow the instructions to edit your YAML code to look like my example in Tutorial 2B. Choose any theme that you like. Leave a blank line under your YAML code, then create a level-1 Heading called “Markdown”. Save and make sure that it will preview/knit. Make a new code chunk. Inside here, you can add all the libraries you need (if you do not have them, you can install them using the tutorial from last week). Start by entering these, but if you need any more packages you can add them here and rerun the code chunk. Run the code chunk a few times to make sure they load with no errors. # Load libraries library(tidyverse) library(dplyr) library(ggpubr) library(Stat2Data) library(corrplot) library(olsrr) library(sf) library(tmap) library(readxl) library(plotly) library(nortest) ## you may need additional libraries. Just add them to the list as you use them. Edit the “code chunk code” at the top of the code chunk so that the code is run and it is shown, but none of the warnings or messages show up (e.g. none of the the friendly text is shown). 7.3 Diagnostic summaries 7.4 Florida Fish Challenge 7.4.1 Important background Small amounts of the element mercury are present in many everyday foods. These do not normally affect your health, but too much mercury can be poisonous. Mercury itself is naturally occurring, but the amounts in the environment have been on the rise from industrialization. The metal can make its way into soil and water, and eventually build up in animals like fish, which are then eaten by people. More details here: https://www.wearecognitive.com/project/extra-narrative/bbc-mercury https://medium.com/predict/mercury-pollution-reaches-the-deep-sea-f59a4938dc7c In the late 1980s, there were widespread public safety concerns in Florida about high mercury concentrations in sport fish. In 1989, the State of Florida issued an advisory urging the public to limit consumption of “top level” predatory fish from Lake Tohopekaliga and connected waters: including largemouth bass (Micropterus salmoides), bowfin (Amia calva), and gar (Lepisosteus spp.). This severely impacted tourism and the economy in the area. Urgent research was required to inform public policy about which lakes needed to be closed. We are going to reproduce part of one study on the topic conducted by T.R. Lange in 1993. Dr Lange and their team took samples from 53 lakes in the Central Florida area. Using water samples collected from each of the lakes, the researchers measured the pH level, as well as the amount of chlorophyll, calcium and alkalinity. The Mercury concentration in the muscle tissue of lake fish was also recorded and standardized to take into account the different ages of fish caught in the lake. Figure 7.1: a. (Left): The mercury food chain in fish.(Wikimedia commons, Bretwood Higman, Ground Truth Trekking) b. (middle) A large bass caught and released in a central Florida lake (https://www.wired2fish.com/news/young-man-catches-releases-huge-bass-from-bank/) c. (right). The location of the lakes in Florida (Google maps) You have been asked to assess whether the alkalinity levels of a lake might impact Mercury levels in largemouth bass. You will be presenting your results to the Florida State Governor in order to set new fishing regulations. She has asked for a report to provide your full thinking and workflow in how you decide on your final model. The units of the your dataset are: Variable Unit Chlorophyll micrograms/Litre, \\(\\mu g/L\\) Alkalinity miligrams/Litre, \\(mg/L\\) pH Unitless Calcium miligrams/Litre, \\(mg/L\\) Mercury Micrograms, \\(\\mu g\\) Age Years 7.4.2 Lab 6 Access the data Optional Hint: If you have lots of different variables in your workspace/environment, you can clear them by clicking on the broom or by typing rm(list=ls()) into the console. Download bassdata.csv from Canvas and read into R as a variable called bass. Question 1 [OPTIONAL BONUS 2%] : Within one of your answers, include a professional looking table using equations where relevant. For example this could be comparing model diagnostic outputs, talking about model parameters, writing up LINE results.. Hint, here is the place I make all my tables, then copy them across: https://www.tablesgenerator.com/markdown_tables. Here is the place I create my equations: https://www.codecogs.com/latex/eqneditor.php 7.4.3 Question 2 - explore data Explore and describe the data. If you are unsure what to do here, think about your credit analysis in lab 2. E.g. How much data is there?, how is that data distributed? What are some summary statistics? Any missing values? Is each variable normally distributed? Remember that you are writing this as a report for the Florida State Governor, so summarise any results you find in the text. Essentially your aim is to explore/describe the data so that the Florida State Governor is happy that the data is robust and you understand it. 7.4.4 Question 3 - response and predictor Identify your response and predictor variables. Create a professional looking scatterplot (including units, axis titles, good formatting etc) and fully describe the plot (in the way that you have done in Lecture 7). As part of your response, explain if Simple Linear Regression might/might not be appropriate in this case. 7.4.5 Question 4 - fit model Fit a Simple Linear Regression model for the problem. Re-plot your scatter plot with the line of best fit. Clearly interpret the estimated model parameters (slope &amp; intercept)/model summary-statistics in the the context of the problem, in a way that would be understandable to a policy maker. For example you could show the regression equation including the parameter values and units in the description. You could talk about the amount of variation explained by the model etc. 7.4.6 Question 5 - regression diagnostics Use regression diagnostics to assess whether the model is appropriate. For example, does it meet the LINE assumptions? (Tutorial 5B) Again, make sure to explain any plots or statistical tests that you use in words (e.g. what are you trying to test for, what did you find out, what did it mean). Assess if there are any outliers, high leverage points Tutorial 6A. 7.4.7 Question 6 - indentify residuals Identify: The name of the lake with highest residual mercury value [4 marks] The name of the lake with highest leverage [4 marks] The name of the lake with highest Cook’s distance [4 marks] 7.4.8 Question 7 - influential points Another policy maker suggested that there are several influential points in the data that should be investigated. From your analysis, do you agree with this comment? Explain your reasoning and provide evidence. 7.4.9 Question 8 - transformation After examining the data, the results were double-checked and it was decided to keep ALL of the data points. Another team member suggested that perhaps the observed residual diagnostics are because the there is a lack of linearity between the two variables of interest. They proposed you should apply a transformation and refit the data. Using the lecture notes/class discussions about possible starting points for transformations, how would you proceed? (i.e. would you transform the response variable, explanatory variable or both? What transformations could you use?) Clearly explain why you came to this conclusion. Note, we will be discussing this in the classes on Monday-15 March and Friday-19 March. (it’s a 2 week lab) 7.4.10 Question 9 - two models After some research, two models were proposed that might fit the data: A log transformation on the explanatory variable (log() command) The square root of the explanatory variable (sqrt() command) Apply the two transformations to the data and choose which one is the best fit. Explain your reasoning referring to any goodness of fit measures that you used. 7.4.11 Question 10 - log transform After talking with some marine biologists, you decide to use the log transformation as it has more physical relevance. Summarise your new model. E.g. make a scatterplot, plot your new line of best fit and summarise the model itself, including the new model equation IN THE CONTEXT OF THE DATA (e.g. what are your coeffients now showing?) Examine the residual diagnostics for your new fit. Explain clearly whether the new model meets each of the assumptions needed for linear regression and assess if there are any influential outliers. Is this model a better fit to the data than the original one? 7.4.12 Question 11 - new lake The Governor recently had a question from a member if the public who went fishing in a new lake that was not part of the study. We know the alkalinity level of that lake was 40mg/L. The member of the public wants to be 99% sure that they won’t exceed the Florida Health Advisory level for Mercury levels in Fish, which is 1 \\(\\mu g\\) of Mercury. Should they eat the fish? Explain your answer and show your evidence for how you came to your conclusion. 7.4.13 Question 12 - health check This question is designed to be more difficult and realistic. I will answer points of clarification, but I will not help anyone work through it before the labs are submitted. However I will award partial marks for workings and how far you get The Florida Health Advisory level for Mercury levels in Fish is 1 \\(\\mu g\\) of Mercury. The Governor has accepted your model and is requiring state-wide alkalinity tests. What is your safety cut-off value of alkalinity for new lakes? (You would like to be 95% sure that you aren’t just seeing this result by chance). Provide evidence of how you got to your answer 7.5 Submitting Lab 6 Remember to save your work throughout and to spell check your writing (next to the save button). Now, press the knit button again. If you have not made any mistakes in the code then R should create a new html file which includes your answers. This can be found in your Lab 6 folder and have a .html ending. Check your html is complete by double clicking on to open it in your web-browser. Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 6. (See the end of Lab 1 for a screenshot) 7.5.1 Lab 6 submission check HTML FILE SUBMISSION - 5 marks RMD CODE SUBMISSION - 5 marks MARKDOWN/CODE/WRITING STYLE - 10 MARKS 10/10 - your report is very professional. There are tables of contents, headings/subheadings, your plots look great, you answer in full sentences and have used the spell check. You have written your answers below the relevant code chunk in full sentences in a way that is easy to find and grade. It’s clear you put thought and effort into writing a good markdown document. I could use this as a class example. You would be comfortable showing it in a job interview. 8/10 - your report is fine on the basics, but not quite as snazzy &amp; is clearly a homework. less - as your report becomes harder to read. QUESTION 1 - 2 marks BONUS (capped at 100%) Your answer to question 1 was in professional table format. QUESTION 2 - 5 marks You have clearly summmarised the data QUESTION 3 - 5 marks Your response and predictor are identified. Your plot looks professional enough that you could give it to the Florida state governor (you can also use fig.cap in the chunk description if you want to add more information that way underneath the plot). You have explained if Simple Linear regression is appropriate. QUESTION 4 - 10 marks You have correctly fitted the simple linear model to the data and interpreted the model output clearly in the text in a way that would be understandable to a policy maker. QUESTION 5 - 9 marks You have correctly used model diagnostics to assess whether the model meets LINE assumptions. You have assessed whether there are outliers, influential points or points with high leverage QUESTION 6 - 6 marks The name of the lake with highest residual mercury value [2 marks] The name of the lake with highest leverage [2 marks] The name of the lake with highest Cook’s distance [2 marks] QUESTION 7 - 5 marks You have discussed whether there are two influential points or what might be going on QUESTION 8 - 5 marks You have discussed which transformations you might apply and why. QUESTION 9 - 10 marks You have transformed the data and compared the two models. QUESTION 10 - 5 marks You have fully summarised the new model and discussed if it is a better fit. QUESTION 11 - 10 marks You have assessed the question from the public about whether they should eat the fish and provided evidence for your answer. QUESTION 12 - 10 marks You have assessed the question about alkalinity safety levels (or as far as you get) &amp; explained your answer. [100 marks total] "],
["lab-7.html", "Chapter 8 Lab 7 8.1 Lab 7 General information 8.2 Lab 7 Setup 8.3 Movie ratings 8.4 Submitting Lab 7", " Chapter 8 Lab 7 8.1 Lab 7 General information Welcome to STAT-462 lab 7. The aim of this week is to: Carry out multiple regression Compare models using stepwise regression This lab also provides less explicit instructions, but builds heavily on previous labs. So if you are not sure what to do, see what I was asking you to do in previous homeworks or in the lecture notes. Each lab builds on the last one. General comments You do not have to submit things you try from the tutorials, you will just be graded on the lab 7 assignment. There is also a canvas discussion board for this lab which will be the fastest place to get an answer. https://psu.instructure.com/courses/2115020/discussion_topics/13706461 8.2 Lab 7 Setup Save all your work and if you haven’t already, create a Lab 7 folder in your STAT-462 folder Create a new R-project called Lab 7 that is linked to your Lab 6 folder (instructions in Tutorial 1B). It will probably open a new version of R-Studio. Create a new Markdown file called “username_Lab7” (for me it will be hlg5155_Lab7) Remove the “friendly text” (see Tutorial 1E if you have no idea what I mean) Follow the instructions to edit your YAML code to look like my example in Tutorial 2B. Choose any theme that you like. Leave a blank line under your YAML code, then create a level-1 Heading called “Markdown”. Save and make sure that it will preview/knit. Make a new code chunk. Inside here, you can add all the libraries you need (if you do not have them, you can install them using the tutorial from last week). Start by entering these, but if you need any more packages you can add them here and rerun the code chunk. Run the code chunk a few times to make sure they load with no errors. # Load libraries library(tidyverse) library(dplyr) library(ggpubr) library(Stat2Data) library(corrplot) library(olsrr) library(sf) library(tmap) library(readxl) library(plotly) library(nortest) ## you may need additional libraries. Just add them to the list as you use them. Edit the “code chunk code” at the top of the code chunk so that the code is run and it is shown, but none of the warnings or messages show up (e.g. none of the the friendly text is shown). 8.3 Movie ratings 8.3.1 Introduction &amp; problem statement Here we will focus on Multiple Linear Regression where there is more than one predictor. You are an analyst for a Hollywood studio. The studio wants to understand how well a movie will perform on the review website Rotten Tomatoes. They have paid you to build a model to predict the movie’s Rotten Tomatoes score. Download the two files from Canvas which contain the data. The file HollywoodMovies2011.csv is the data itself. includes information on movies that came out of your Hollywood Studio in 2011. The dataset contains the Rotten Tomatoes score plus five predictor variables. Download it from Canvas and read it into R as a variable called movies. The meta data (the data explaining what the spreadsheet shows), is stored in HollywoodMovies2011MetaData.txt. Download it from Canvas. You do not need to read it into R, but take a look as it provides vital information about what the data is showing. 8.3.2 Question 1 Explanation and guide I want to clear up some misconceptions about the first question I ask in every lab. Some people have asked me why are they bothering to do this because we don’t seem to use the results later on (for example about testing for normality). The answer is that this isn’t a series of tests we are applying to this, instead this question every lab is a chance to explore the data itself. Let’s split the guide into what you need to do vs what you need to write up. 8.3.2.1 What you need to do First, “Question 1” is about deciding whether this dataset is going to help you answer your research question. You also need to understand as much as possible where the data comes from and who collected it. Think about the topic and consider your population. - Will this dataset be representative of your population? - If not, do you need to narrow the question or find more data? # For example, if your problem and population are measuring the height of Penn State students - and your sample only contains people on the sports teams, you might not have a representative sample. Second, “Question 1” is about is understanding and exploring the data yourself. This is known as Quality Control, or Exploratory Data Analysis (EDA). For example you could check Are all your numeric variables numeric? Are there columns reading as character that should be numeric? Sometime people enter the number 0 as the letter O by accident, which will completely confuse R. If your data includes categories, are the spellings/capitalization consistent (e.g. Red and red would read as different categories) Are there missing values? Are there “stupid” values? (a height of -7m.. or 700m..) which might want to be replaced with NA (missing) Are any entire rows of data which are clearly wrong and should be removed? Do things have the means and range you might expect? Are the units right? Third, we want to know if the data is suitable for use in a multiple regression model. Explore your variables in more detail, with a focus on your response. Is the data normally/symmetrically distributed? Or skewed? If the response is not normally distributed, that is a red flag that the regression might not meet LINE later on. Are there outliers? Are there clusters? Does the mean and range make sense for the units and your understanding of the problem? Is there anything that will affect your analysis later on? If you only have a few predictors (especially if just x vs y), consider exploring the distribution of all variables in detail. If you have many predictors (like this lab!), describe the response variable in detail, then use the correlation plot etc to describe predictors - see question 2. When I say describe, I mean, examine the average value, some idea of spread (IQR, standard deviation), note any unusual values and the data distribution (normality/skew/clusters). If your data is spatial, map it! To do much of this you could use either the summary function or the skim command in the skimr package (remember you might need to install it using install.packages(skimr) and then load it using library(skimr)). For example, in this dataset, there is a row which is clearly incorrect. This stage of “explore the data” will help you find it. For the sake of the lab grades, please leave the code you use to create your results. In a professional report you might want to remove some/all of it and just report your summary of the final dataset ready for use. 8.3.2.2 What you need to write up OK, so above are activities I commonly do to understand the data. To write up my answer to this analysis, I would normally do something like this. Re-iterate the problem statement. Use that to describe the population under study. Describe the meta-data of the sampled dataset: What variables are there? (and units!). How many observations are there? Summarise the meta data in a neat table in R e.g. what is your response variable (and units!), what are your predictor/explanatory variables (and units!) If the units are ambiguous, remember to include more details. E.g. in your case, several columns are percentages. Are they recorded as numbers from 0-1 or numbers from 0-100? If available, how was the data collected and by whom? If available, what is the reference for the dataset? How did you access it? Is the dataset representative of your population? E.g. data on chickens collected in the year 1980 might (or might not!) be representative of conditions now. Describe the quality control you performed. If you only have a few predictors (especially if just x vs y), consider exploring the distribution of all variables in detail. If you have many, describe the response in detail, then use the correlation plot etc to describe predictors. When I say describe, I mean, the average value, some idea of spread (IQR, standard deviation), any or unusual values and the data distribution (normality/skew/clusters) Essentially, your job is to convince the customer that the data is fit for use and to describe the process from data collection (or when you accessed it), to the point where you start the analysis. 8.3.3 Question 1 - EDA Conduct any relevant quality control on the dataset as described in the guide. If you have reason to remove data, then you are allowed to do this with justification. Then, as described in the guide above, describe the problem and the data you have to solve it. Describe the response variable in detail. 8.3.4 Question 2 - Correlation plots As described above, it can be difficult to examine many predictors in a large dataset. One tool we have to do this is the correlation plot. You can see how to create one in Tutorial 7A) Use the correlation plot to describe the relationship between your response variable and each of your predictors. Which predictors do you think will have the strongest impact on a movie’s Rotten Tomatoes score. 8.3.5 Question 2B [OPTIONAL BONUS 2%] - corrplots Instead of the correlation plots given in Tutorial 7A, choose a new format one from https://www.r-graph-gallery.com/correlogram.html and get it up and working with your data. 8.3.6 Question 3 - MLR Use Tutorial 7B to answer this question. HINT, BEFORE YOU RUN THIS - IF YOU HAVEN’T ALREADY DONE THIS, GO INTO THE CSV FILE IN EXCEL AND REMOVE THE “TEST TEST” ROW. SAVE THE EXCEL FILE AS SOMETHING NEW, THEN READ THE NEW EXCEL FILE INTO R AND CARRY ON. Fit a full first order regression model to the data, with RottenTomatoes as your response and your predictors are all the other variables. Summarise the model in your write up and write out the model equation (including the coefficients as numbers) &amp; summarising units afterwards Look at the model summary and write how much variability in the Rotten Tomatoes score is explained by your model. Note, 1st order means that we don’t include polynomials in our predictors e.g.This is a first order model: \\(y = \\beta_0 + \\beta_{1}x_1 + \\beta_{2}x_2\\). e.g.This is a second order model: \\(y = \\beta_0 + \\beta_{1}x_1 + \\beta_{3}{x_1}^2 + \\beta_{2}x_2\\) 8.3.7 Question 4: Model assumptions Use Tutorial 7C and Tutorial 5B (or lecture 19) to assess if your model residuals meet LINE assumptions. 8.3.8 Question 5: Outliers Use Tutorial 7C and Tutorial 6A to assess if any movie appears to be an influential outlier. If a movie does fit the criteria of influential outlier, do not remove anything from the dataset. Instead, in your write-up, please provide its name and justify why you think it is influential. 8.3.9 Question 6 - Assessing overall model fit Write a hypothesis test using the F-Statistic/ANOVA to test whether our model contains at least one predictor useful in predicting Rotten Tomatoes score. Hint, we will cover this in lectures, but also read this: https://online.stat.psu.edu/stat462/node/137/ 8.3.10 Question 7 - Partial Slopes “The test-results for partial slopes” is a fancy way of saying “look at the T-test result and corresponding p-value of each variable in the model summary”. Essentially, they are the likelihood of that variable being an important part of the model IF EVERYTHING ELSE WAS HELD CONSTANT. By looking at the test results for the partial slopes (at a 10% level of significance), identify any variable/s you would like to drop from your model fit Provide reasons for your choice(s). Does this meet your expectations from the correlation matrix? You do not have to write down any steps for hypothesis testing here, but you do need to justify your decision. 8.3.11 Question 8: Re-fit Model Fit a new model by eliminating the variables you decided to drop in the previous question. Write down the estimated regression line for your new model. 8.3.12 Question 9: Compare the models Compare your two models using \\(AdjR^2\\) and AIC. Which model do you consider to be a better fit and why? Why did I ask you to choose adjusted \\(R^2\\) not standard \\(R^2\\)? 8.3.13 Question 10: Predict a new movie’s score The studio has just trialed a new movie with details: Variable Name Value Name “Hunt of the Killer Cactus” AudienceScore 59% TheatersOpenWeek 5 cinemas BOAverageOpenWeek $147262 DomesticGross 16.38 million USD Profitability 88.31% of the budget recovered in profits What is your estimate of this new movie’s Rotten Tomatoes score? What is your 95% range of uncertainty on this estimate? Tutorial 7D might be useful in this question. Hint, remember when you write up your answer to make your answer realistic. For example, what is the maximum value your response could be? 8.3.14 Question 11: Find the “best” model Finally, there are many combinations of predictors that we could use to predict our response. We want to find the best model possible but we also don’t want to overfit. So far, we manually compared two models. In fact there is a way to compare all the combinations of predictors. This is using the ols_step_best_subset() command. Run this on your original linear model fit (the one including all the variables). e.g. ols_step_best_subset(mymodel). Describe what the “best subset” method is doing. Hint, we will go over this in lectures, but also https://online.stat.psu.edu/stat501/lesson/10/10.3 Use the subset method to assess the optimal fit using at least 3 goodness of fit measures. What is your final conclusion? 8.4 Submitting Lab 7 Remember to save your work throughout and to spell check your writing (next to the save button). Now, press the knit button again. If you have not made any mistakes in the code then R should create a new html file which includes your answers. This can be found in your Lab 7 folder and have a .html ending. Check your html is complete by double clicking on to open it in your web-browser. Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 7. (See the end of Lab 1 for a screenshot) 8.4.1 Lab 7 submission check HTML FILE SUBMISSION - 5 marks RMD CODE SUBMISSION - 5 marks MARKDOWN/CODE/WRITING STYLE - 10 MARKS 10/10 - your report is very professional. There are tables of contents, headings/subheadings, your plots look great, you answer in full sentences and have used the spell check. You have written your answers below the relevant code chunk in full sentences in a way that is easy to find and grade. It’s clear you put thought and effort into writing a good markdown document. I could use this as a class example. You would be comfortable showing it in a job interview. 8/10 - your report is fine on the basics, but not quite as snazzy &amp; is clearly a homework. less - as your report becomes harder to read. QUESTION 1 - 10 marks BONUS (capped at 100%) You have explored the data using the guide, conducted quality control where you removed the observation that was clearly wrong and written up your results clearly. QUESTION 2 - 5 marks You have created the correlation matrix plot and sensitively described the relationship between your response and your predictors. QUESTION 2B - 2 marks - OPTIONAL You found a new way of making the correlation plot not shown in the tutorial. QUESTION 3 - 10 marks You created the model correctly. In your write up you have summarised the model equation (including the coefficients as numbers) &amp; summarising units afterwards. You have produced a model summary and written how much variability in the Rotten Tomatoes score is explained by your model. QUESTION 4 - 8 marks You have assessed whether the model meets LINE assumptions (2 for each) QUESTION 5 - 7 marks You have assessed whether there are outliers and whether they are influential. You have identified any movies which are influential. QUESTION 6 - 10 marks You have correctly conducted a hypothesis test to assess model fit. QUESTION 7 - 5 marks You have assessed which variables do not add to the model using partial slopes. QUESTION 8 - 5 marks You have correctly refitted and interpreted the model. QUESTION 9 - 5 marks You have compared the models using 3 goodness of fit tests. QUESTION 10 - 7 marks You have correctly predicted the rotten tomatoes score of Hunt of the killer cactus. QUESTION 11 - 8 marks You have found the “optimal model” and commented on what the best subset command is doing,. [100 marks total] "]
]
